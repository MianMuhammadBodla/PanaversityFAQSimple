{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccAY2OXz6MOx",
        "outputId": "5644625d-f1c7-4199-e499-8ea2d79fc202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.8.3)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.10/dist-packages (0.3.25)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.2.60-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.10)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.155.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.25.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.10.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai) (1.25.0)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (0.2.3)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (24.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (9.0.0)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.4 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.9-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.48-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.4->langgraph) (1.1.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.10/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.27.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.42.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.2.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.68.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
            "Downloading langchain_google_genai-2.0.7-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.2.60-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading langgraph_checkpoint-2.0.9-py3-none-any.whl (37 kB)\n",
            "Downloading langgraph_sdk-0.1.48-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, uvicorn, starlette, langgraph-sdk, fastapi, langgraph-checkpoint, langgraph, langchain-google-genai\n",
            "Successfully installed fastapi-0.115.6 filetype-1.2.0 langchain-google-genai-2.0.7 langgraph-0.2.60 langgraph-checkpoint-2.0.9 langgraph-sdk-0.1.48 starlette-0.41.3 uvicorn-0.34.0\n"
          ]
        }
      ],
      "source": [
        "pip install google-generativeai langchain-google-genai langchain-core langgraph fastapi uvicorn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langchain_google_genai langchain_core langgraph"
      ],
      "metadata": {
        "id": "IHKxSDEcHpEo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "2jYeUXcsHqgl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "LANGCHAIN_API_KEY = userdata.get('LANGCHAIN_API_KEY')"
      ],
      "metadata": {
        "id": "BCXMWe89Hq-X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = \"your-gemini-api-key\""
      ],
      "metadata": {
        "id": "UOsPPLDyFikk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "# This will be a tool\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Adds a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a + b\n",
        "\n",
        "def divide(a: int, b: int) -> float:\n",
        "    \"\"\"Divide a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a / b\n",
        "\n",
        "tools = [add, multiply, divide]\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", api_key = GEMINI_API_KEY)\n",
        "llm_with_tools = llm.bind_tools(tools)"
      ],
      "metadata": {
        "id": "hc8QAL7vH7O5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Optional\n",
        "import os\n",
        "from datetime import datetime\n",
        "import google.generativeai as genai\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import MessagesState\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from pydantic import BaseModel\n",
        "import json\n",
        "\n",
        "# Configure Gemini\n",
        "genai.configure(api_key=\"YOUR_GEMINI_API_KEY\")\n",
        "\n",
        "# Knowledge base\n",
        "PANAVERSITY_KNOWLEDGE = \"\"\"\n",
        "\n",
        "Certified Agentic and Robotic AI Engineer\n",
        "Master the Future: Getting You Ready For The $100 Trillion AI Industrial Revolution\n",
        "\n",
        "                     Agentic AI                                                     Graph Databases/GQL\n",
        "\n",
        "\n",
        "Humanoids\n",
        "\n",
        "\n",
        "Learn to Build Autonomous AI Agents, Humanoids, and Fine-Tune LLMs\n",
        "Version: 16.1 (Implementation and adoption starting from January, 2025)\n",
        "\n",
        "Must Watch Video To Get Started: Customer Experience Trends for 2025: The Rise of AI Agents and Agentic AI\n",
        "Watch the Rise of Agentic AI Video Presentation: https://www.facebook.com/share/v/1Q5ZmFBx7u/\n",
        "Note: If you have difficulty in watching this video install Opera Browser after you have activated its builtin VPN\n",
        "The Rise of the Agentic AI Slides: https://bit.ly/4hTqT4G\n",
        "The Certification Discussion Podcast: https://youtu.be/ViRWA4wLI8k\n",
        "Detailed Class Schedule for this Quarter: https://bit.ly/piaic-dec-sch\n",
        "Certification Program Review by ChatGPT:\n",
        "https://chatgpt.com/share/6732a6f1-a3c4-8001-99cb-1b272c3b3881\n",
        "\n",
        "The leading technological trends today include Agentic AI, Physical AI, Knowledge Graphs, and Cloud Native and distributed computing technologies. Agentic AI refers to AI systems designed to autonomously perceive, reason, and act to achieve specific objectives, often through iterative decision-making and learning. Agents can be either software-based or robotic. Our initial focus is on software agents, such as Vertical LLM Agents and the development and deployment of SaaS applications. Eventually, we'll turn our attention to humanoid robots and Physical AI, aiming to bridge the gap between digital intelligence and physical capabilities by creating systems that understand and interact with the world in a human-like manner. Cloud Native technology provides a scalable and reliable platform for running applications, while AI imbues these applications with intelligent, human-like features. A knowledge graph is a structured representation of real-world entities and their relationships, organised as nodes (entities) and edges (relationships) in a graph format. This enables both humans and machines to understand, integrate, and reason about complex, interconnected data from various sources. Our goal is to train you to become an exceptional global developer in Cloud Native, Distributed Computing, Knowledge Graphs, Agentic AI, and Physical AI.\n",
        "\n",
        "Material to Understand the Coming Agentic AI Age:\n",
        "Agentic AI Explained\n",
        "AI Agents Explained Like You're 5\n",
        "AI Is About To FLIP Your Life Upside Down\n",
        "The Future Is Agentic\n",
        "The agent economy\n",
        "Why Vertical LLM Agents Are The New $1 Billion SaaS Opportunities\n",
        "Vertical AI Agents Could Be 10X Bigger Than SaaS\n",
        "OpenAI's Path to AGI | Five Levels of Intelligence\n",
        "AI Agents: Are We Ready For Machines That Make Decisions?\n",
        "Function calling\n",
        "Generative AI’s Act o1\n",
        "Watch AGI could Double GDP\n",
        "The INSANE Race for AI Humanoid Robots\n",
        "The AI agents stack\n",
        "\n",
        "This core program duration is one and a half years, if you take one course at a time and equips you with the skills to thrive in the age of Generative, Agentic, and Physical AI, and cloud native distributed computing. However, you can reduce the duration of the program if you take multiple courses in a quarter. You will become an expert Custom GPT, AI Agent, and Humanoid Robotics Developer. The program is divided into two levels: core level and professional level. Students will be able to start working after completing the core level. They will continue their professional level studies while working.\n",
        "Why This Program?\n",
        "Cutting-Edge Skills: Develop in-demand skills to build intelligent, scalable cloud applications using Generative AI and Cloud Native technologies.\n",
        "Industry-Ready: Prepare for global certifications, startup and freelance opportunities after just six months.\n",
        "Future-Proof Your Career: Stay ahead of the curve in a rapidly evolving tech landscape.\n",
        "What You'll Learn:\n",
        "Multi AI Agent Systems and Custom GPTs: Learn to fine-tuning foundational AI models, and market them in GPT stores. Learn key principles of designing effective AI agents, and organising a team of AI agents to perform complex, multi-step tasks. Build Knowledge Graphs. Apply these concepts to automate common business processes.\n",
        "Physical AI and Humanoid Robotics: We will learn to design, simulate, and deploy advanced humanoid robots capable of natural interactions.\n",
        "Develop AI Powered Microservices: Master Python, build APIs using FastAPI, SQLModel, Postgres, Kafka, Kong, and leverage cutting-edge GenAI APIs like OpenAI, and Open Source AI LLMs.\n",
        "Cloud Native Expertise: Design and deploy cloud-native pipelines, and microservices using Docker, DevContainers, TestContainers, Kubernetes, Terraform, and GitHub Actions.\n",
        "Distributed System Design: Designing systems that run on multiple computers (or nodes) simultaneously, interacting and coordinating their actions using Ray.\n",
        "Designing AI Solutions using Design Thinking and Behaviour Driven Development (BDD): We will learn to leverage these methodologies to create AI solutions that are not only technically sound but also highly user-centric and aligned with real-world needs.\n",
        "Fine-Tuning Open-Source Large Language Models using PyTorch, and Fast AI: We will learn to fine-tuning of open-source Large Language Models (LLMs) like Meta LLaMA 3 using PyTorch, Ray, and Fast AI, with a focus on cloud-native training and deployment. We will set up development environments, preprocess data, fine-tune models, and deploy them using cloud native platforms.\n",
        "Flexible Learning:\n",
        "Earn While You Learn: Start freelancing or contributing to projects after the third quarter.\n",
        "Also Focus on Communication Skills:\n",
        "Technical + Communication: You can negotiate any reality you want\n",
        "The Level-Based Structure of the Panaversity AI Program::\n",
        "Our program’s level structure transforms the learning process into a progressive journey. The levels are designed to ensure that participants thoroughly grasp and demonstrate proficiency in each stage before advancing to the next. The process starts by learning Python AI and  progressively moves from Agentic AI, to Cloud Native AI, then to Physical and Robotic AI, culminating in Customizing LLMs. Each level builds on the knowledge and skills from the previous one, increasing in complexity and depth.\n",
        "\n",
        "I. Autonomous Agentic and Robotic AI Core Level\n",
        "AI-101: Modern AI Python Programming\n",
        "The main focus in this course will be on mastering the fundamentals of Modern Python with Typing, the go-to language for AI and using AI to write Python Programs. We will then move to understanding the basics of GenAI and Prompt Engineering. In the end of the program we will understand the basics of Linux, Docker, VSCode, Devcontainer, and GitHub.\n",
        "Certification:\n",
        "Certified Professional Python Programmer (CPPP1)\n",
        "Learning Repo: https://github.com/panaversity/learn-cloud-native-modern-python\n",
        "Prerequisite: None\n",
        "AI-201: Fundamentals of Agentic AI\n",
        "In this quarter, students will embark on a comprehensive journey into the realms of Generative AI and Agentic AI. The curriculum begins with an introduction to these foundational concepts, establishing a solid understanding of their principles and distinctions. Students will then engage in hands-on development of custom GPTs, facilitating a deeper grasp of AI functionalities and applications. A significant portion of the quarter is dedicated to mastering Prompt Engineering, emphasising the creation of effective prompts to optimise AI outputs. Leveraging the user-friendly CrewAI framework, students will develop AI agents, applying their theoretical knowledge to practical scenarios. The course also delves into the construction of Knowledge Graphs using Graph Databases and GQL, equipping students with the skills to organise and query complex data structures. Additionally, the semester covers Agentic Payments, exploring the integration of AI agents in financial transactions and payment systems.\n",
        "Learning Repo:\n",
        "https://github.com/panaversity/learn-agentic-ai/\n",
        "Covers Chapters from -01 to 11 or the Repo\n",
        "Prerequisite: AI-101\n",
        "\n",
        "\n",
        "AI-202: Advanced Agentic AI Engineering\n",
        "Building upon the foundational knowledge acquired in AI-201, this quarter focuses on advanced Agentic AI engineering using more sophisticated frameworks. Students will delve into LangGraph and LangChain, gaining proficiency in these powerful tools to create complex AI agents. The curriculum emphasises the development of sophisticated AI agents capable of performing intricate tasks and decision-making processes. Through a combination of theoretical instruction and practical projects, students will enhance their capabilities in designing, implementing, and deploying advanced AI solutions, preparing them for challenges in the evolving field of Agentic AI.\n",
        "The course will then transition to frontend development, where learners will be introduced to Next.js and its powerful features for building dynamic web applications. By incorporating TypeScript, participants will enhance their development process with strong typing and advanced features. This part concludes with a project, allowing learners to develop a complete AI agent frontend, combining their Knowledge Graphs with modern web technologies for a seamless user experience.\n",
        "Learning Repo:\n",
        "https://github.com/panaversity/learn-agentic-ai/\n",
        "Covers Chapters 12 and onwards\n",
        "Prerequisite: AI-101, AI-201\n",
        "AI-301: Cloud Native AI Microservices\n",
        "Build scalable AI Powered APIs using FastAPI, GQL, Neo4j, Kafka, Kong, GenAI APIs like OpenAI Chat Completion APIs, Assistant APIs, LangChain and Open Source AI LLMs, develop them using Containers and Dev Containers, and deploy them using Docker Compose locally and Kubernetes Powered Serverless Container Services on the cloud.\n",
        "We will also learn to integrate design thinking and Behavior-Driven Development (BDD) in developing AI systems. We will learn to create AI solutions that are deeply aligned with user needs and expectations. Design thinking ensures a thorough understanding of the user and problem space, while BDD provides a structured approach to defining and validating the desired behaviours of the AI system. Together, these methodologies lead to the development of AI solutions that are not only technically robust but also highly user-centric and effective in solving real-world problems.\n",
        "Certifications:\n",
        "Neo4j Certified Professional\n",
        "Confluent Certified Developer for Apache Kafka (CCDAK)\n",
        "Design Thinking Professional Certificate (DTPC)\n",
        "Test and Behavior Driven Development (TDD/BDD)\n",
        "\n",
        "Learning Repo:\n",
        "https://github.com/panaversity/learn-cloud-native-ai-powered-microservices/\n",
        "We Will Be Using Microsoft Azure as our Default Cloud Platform\n",
        "Amazon is still the cloud king based on market share. But many analysts agree: In the battle for the cloud, AI is now a game-changer — and Amazon's main competitors, particularly Microsoft, have the momentum. In our program we will be using Azure as our default provider for teaching and deployment.\n",
        "Prerequisite: AI-101\n",
        "\n",
        "AI-451: Physical and Humanoid Robotics AI\n",
        "Artificial intelligence (AI) has experienced remarkable advancements in recent years. However, the future of AI extends beyond the digital space into the physical world, driven by robotics. This new frontier, known as “Physical AI,” involves AI systems that can function in the real world and comprehend physical laws. This marks a notable transition from AI models confined to digital environments. Humanoid robots are poised to excel in our human-centred world because they share our physical form and can be trained with abundant data from interacting in human environments.\n",
        "This course provides an in-depth exploration of humanoid robotics, focusing on the integration of ROS 2 (Robot Operating System), Gazebo Robot Simulator, and NVIDIA Isaac™ AI robot development platform. Students will learn to design, simulate, and deploy advanced humanoid robots capable of natural interactions. The curriculum covers essential topics such as ROS 2 for robotic control, simulations with Gazebo and Unity, and using OpenAI’s GPT models for conversational AI. Through practical projects and real-world applications, students will develop the skills needed to drive innovation in humanoid robotics.\n",
        "Learning Repo: https://github.com/panaversity/learn-physical-ai-humanoid-robotics\n",
        "Prerequisite: AI-101\n",
        "AI-461: Distributed AI Computing\n",
        "Ray is the AI Compute Engine. Ray manages, executes, and optimises compute needs across AI workloads. It unifies infrastructure via a single, flexible framework—enabling any AI workload from data processing to model training to model serving and beyond. This course provides an in-depth exploration of distributed computing using Ray, a framework for building and scaling distributed Python applications. Students will learn to develop, deploy, and optimise distributed systems using Ray, with applications in machine learning, data processing, and reinforcement learning. Ray is an open-source distributed computing framework designed to simplify the development and scaling of machine learning (ML) and Python applications.\n",
        "The following examples highlight Ray’s widespread adoption and its effectiveness in enhancing scalability, performance, and cost-efficiency in AI and machine learning workloads.\n",
        "OpenAI: Utilises Ray to train large models, including ChatGPT, enabling faster iteration at scale.\n",
        "Amazon Web Services (AWS): Employs Ray to enhance scalability, reduce latency by over 90%, and improve cost efficiency by over 90% in specific applications.\n",
        "Ant Group: Deployed Ray Serve on 240,000 cores for model serving, achieving a peak throughput of 1.37 million transactions per second during high-demand periods.\n",
        "Uber: Leverages Ray to rapidly pretrain, fine-tune, and evaluate large language models (LLMs).\n",
        "Instacart: Uses Ray to run deep learning workloads 12 times faster, reduce costs by 8 times, and train models on 100 times more data. (Ray)\n",
        "Samsara: Implemented Ray to scale the training of deep learning models to hundreds of millions of inputs, accelerating deployment and cutting inference costs by 50%.\n",
        "Cohere: Utilises Ray to simplify the development of scalable distributed programs for large language model pipelines. (Ray)\n",
        "\n",
        "Prerequisite: AI-101\n",
        "\n",
        "II. Professional Level\n",
        "AI-500: AI Ethics and Governance: Principles and Practices\n",
        "In the rapidly evolving field of artificial intelligence, understanding the ethical implications and governance frameworks is crucial. This course delves into the core principles of AI ethics, including fairness, transparency, accountability, and privacy. Participants will explore the societal impacts of AI across various sectors, examine international and national governance structures, and learn strategies for integrating ethical considerations into AI design and deployment. Through case studies and a capstone project, students will gain practical skills to navigate and address ethical challenges in AI, preparing them to lead responsible AI initiatives in their organisations.\n",
        "AI-501: Distributed Machine Learning\n",
        "Generative AI tools like ChatGPT, Gemini, and DALL-E have revolutionised our professional landscape. This hands-on course guides you through the exciting distributed process of building and training AI models using Python and the versatile, open-source PyTorch and Ray frameworks. You’ll delve into the core concepts of Generative Adversarial Networks (GANs), Transformers, Large Language Models (LLMs), variational autoencoders, diffusion models, and more. Along the way, you’ll gain practical experience and a deep understanding of these cutting-edge technologies.\n",
        "\tLearning Repo: https://github.com/panaversity/genai-with-pytorch\n",
        "Prerequisite: AI-101, AI-461\n",
        "AI-502: Customising Open Source LLMs\n",
        "This comprehensive course is designed to guide learners through the process of fine-tuning open-source Large Language Models (LLMs) such as Meta LLaMA 3 using PyTorch, with a particular emphasis on cloud-native training and deployment. The course covers everything from the fundamentals to advanced concepts, ensuring students acquire both theoretical knowledge and practical skills.\n",
        "The journey begins with an introduction to LLMs, focusing on their architecture, capabilities, and the specific features of Meta LLaMA 3. Next, the course dives into PyTorch fundamentals, teaching students how to perform basic operations with tensors and build simple neural networks. This foundation is crucial for understanding the mechanics behind LLMs. Data preparation is a crucial aspect of training models. The course covers comprehensive data collection and preprocessing techniques, such as tokenization and text normalisation. These steps are essential for preparing datasets suitable for fine-tuning LLMs like Meta LLaMA 3. Through practical exercises, students learn how to handle and preprocess various types of text data, ensuring they can prepare their datasets for optimal model performance.\n",
        "Fine-tuning Meta LLaMA 3.2 with PyTorch forms a significant part of the course. Students will delve into the architecture of Meta LLaMA 3, learn how to load pre-trained models, and apply fine-tuning techniques. The course covers advanced topics such as regularisation and optimization strategies to enhance model performance. Practical sessions guide students through the entire fine-tuning process on custom datasets, emphasising best practices and troubleshooting techniques.\n",
        "A critical aspect of this course is its focus on cloud-native training and deployment using Nvidia NIM. Furthermore, students learn how to deploy models using Docker and Kubernetes, set up monitoring and maintenance tools, and ensure their models are scalable and efficient.\n",
        "To round off the learning experience, the course includes an in-depth segment on exporting models for inference and building robust inference pipelines. Students will deploy models on cloud platforms, focusing on practical aspects of setting up monitoring tools to maintain model performance and reliability.\n",
        "The course culminates in a capstone project, where students apply all the skills they have learned to fine-tune and deploy Meta LLaMA 3 on a chosen platform. This project allows students to demonstrate their understanding and proficiency in the entire process, from data preparation to cloud-native deployment.\n",
        "Learning Repo:\n",
        "https://github.com/panaversity/learn-fine-tuning-llms\n",
        "Prerequisite: AI-101, AI-461, AI-501\n",
        "AI-651: Advanced Cloud Native and Distributed AI Computing\n",
        "Master Kubernetes, Ray, Terraform, and GitHub Actions to deploy your AI pipelines, APIs, microservices, and open source models in the cloud. We will cover distributed system design involving creating AI systems that are distributed across multiple nodes, focusing on scalability, fault tolerance, consistency, availability, and partition tolerance.\n",
        "Certifications:\n",
        "Certified Kubernetes Application Developer (CKAD)\n",
        "HashiCorp Certified: Terraform Associate\n",
        "Learning Repo: https://github.com/panaversity/learn-kubernetes\n",
        "Prerequisite: AI-101, AI-301, AI-461\n",
        "\n",
        "III. Vertical Specialization Level (Optional)\n",
        "Students will have the option of selecting one of the following specialisations, details available at the end of FAQs:\n",
        "\n",
        "AI-701: Healthcare and Medical Agentic AI\n",
        "AI-702: Web3, Blockchain, and Agentic AIIntegration\n",
        "AI-703: Metaverse, 3D, and Agentic AI Integration\n",
        "AI-704: Agentic AI for Accounting, Finance, and Banking\n",
        "AI-705: Agentic AI for Engineers\n",
        "AI-707: Agentic AI for Sales and Marketing Specialization\n",
        "AI-708: Agentic AI for Automation and Internet of Things (IoT)\n",
        "AI-709: Agentic AI for Cyber Security\n",
        "\n",
        "Zia Khan, CEO Panaversity\n",
        "MSE, MBA, MAC, MA, CPA, CMA\n",
        "https://www.linkedin.com/in/ziaukhan/\n",
        "Common Questions (FAQs) with Detailed Answers\n",
        "\n",
        "What is a Certified Agentic and Robotic AI Engineer?\n",
        "A Certified Agentic and Robotic AI Engineer is a professional skilled in the development of autonomous, AI-driven systems that can act and make decisions independently (agentic AI) and physical systems that interact with the physical world (robotic or physical AI). This certification program, offered by Panaversity, equips participants with expertise in building and deploying autonomous software agents, humanoid robots, and fine-tuning large language models (LLMs) for specific applications.\n",
        "\n",
        "The program covers skills across several domains:\n",
        "\n",
        "Agentic AI: Focus on AI systems that can autonomously learn, perceive, reason, and act, including multi-agent AI systems, AI-powered SaaS solutions, and knowledge graphs.\n",
        "Humanoid Robotics and Physical AI: Training on designing, simulating, and deploying robots capable of interacting with humans naturally using platforms like ROS 2 and NVIDIA Isaac.\n",
        "Cloud Native and Distributed Computing: Includes building scalable AI-powered microservices, leveraging Docker, Kubernetes, and Ray for cloud-native and distributed applications.\n",
        "Custom GPT and LLM Fine-Tuning: Students learn to customize LLMs, using tools like PyTorch and FastAI, for specific applications.\n",
        "AI Ethics and Governance: To prepare participants for real-world deployment, the program includes principles and practices for responsible AI use.\n",
        "\n",
        "This comprehensive curriculum enables students to become leaders in the $100 trillion AI-driven industrial revolution, blending technical prowess with ethical awareness and a future-focused approach to the development and application of AI in both virtual and physical domains￼.\n",
        "\n",
        "How valuable can the Certified Agentic and Robotic AI Engineer be in the new age of AI?\n",
        "The Certified Agentic and Robotic AI Engineer certification positions professionals at the forefront of the rapidly expanding AI industry, bridging critical skills in autonomous agents, humanoid robotics, and applied generative AI. Here’s why it’s especially valuable in the new AI age:\n",
        "\n",
        "a. High Demand for AI Talent in Emerging Fields\n",
        "\n",
        "With the rise of agentic AI (systems that can autonomously act and make decisions) and physical AI (robots capable of interacting in human environments), industries from healthcare to manufacturing, finance, and more require specialised professionals who can design, develop, and deploy these solutions. The certification addresses these needs by training engineers to implement cutting-edge AI in real-world applications.\n",
        "\n",
        "b. Market Need for Multi-Disciplinary AI Engineers\n",
        "\n",
        "This program combines expertise in cloud-native distributed computing, AI ethics, and generative AI models with practical robotic engineering. Such a multidisciplinary skill set is rare and in high demand, especially as industries increasingly integrate AI into complex systems that require intelligent automation and decision-making capabilities.\n",
        "\n",
        "c. Potential for Lucrative Careers and Entrepreneurship\n",
        "\n",
        "The certification prepares professionals to either secure high-paying roles or start their own ventures, given the growing demand for AI-driven applications and the ability to deploy them effectively. With cloud-native and agentic AI skills, certified engineers can create specialised applications, SaaS products, or even bespoke AI agents tailored for specific sectors, all of which are lucrative markets. The annual salary for AI professionals, especially those with expertise in cloud-native and generative AI, can range from $150,000 to over $200,000, depending on experience and region ￼.\n",
        "\n",
        "d. Future-Proofing with Robotics and Physical AI\n",
        "\n",
        "Robotics, especially humanoid robots powered by AI, are expected to become more prevalent in the workforce, from automated service roles to advanced manufacturing and healthcare. This program’s focus on humanoid robotics, ROS 2, and NVIDIA Isaac platforms enables graduates to contribute to this pivotal transition, making them invaluable as more businesses adopt physical AI solutions.\n",
        "\n",
        "e. Aligning with the Fourth Industrial Revolution\n",
        "\n",
        "AI, especially generative and agentic AI, is part of what some are calling the Fourth Industrial Revolution—a shift expected to transform industries at a $100 trillion scale by 2030. Certified engineers from this program are positioned to drive innovations that align with this transformation, making their skills and knowledge a vital component of the global economy’s future.\n",
        "\n",
        "In short, the Certified Agentic and Robotic AI Engineer is a credential that prepares professionals not only for immediate high-value opportunities but also for leadership roles in AI’s next evolution. This makes it a highly valuable certification as AI expands its impact across virtually every industry ￼.\n",
        "\n",
        "\n",
        "\n",
        "What is the potential for Certified Agentic and Robotic AI Engineers to start their own companies and become successful startup founders?\n",
        "\n",
        "The Certified Agentic and Robotic AI Engineer credential equips professionals with unique skills that position them as prime candidates for launching successful startups in the AI-driven market. Here’s why certified professionals have a strong potential for entrepreneurial success:\n",
        "\n",
        "a. Emerging Market Demand for Autonomous Solutions\n",
        "\n",
        "With AI agents, robotic systems, and generative AI applications rapidly becoming integral across industries, there is substantial demand for startups that can deliver these technologies in tailored, high-impact solutions. Certified engineers have the knowledge to develop autonomous agents and AI systems for niche markets, such as healthcare, finance, logistics, or personal assistance, where custom solutions can command premium pricing.\n",
        "\n",
        "b. Leverage Agentic AI and Physical AI Skills for Differentiation\n",
        "\n",
        "The certification program’s dual focus on agentic AI (autonomous software agents) and physical AI (robots capable of real-world interactions) provides a distinct edge in the market. Founders with these skills can differentiate themselves by creating products that bridge digital and physical experiences—such as customer service bots that interact seamlessly with customers or humanoid robots designed for human environments.\n",
        "\n",
        "c. Versatile Skills for Lean Startup Operations\n",
        "\n",
        "Certified engineers are trained across the full stack of AI-powered applications, including cloud-native architectures, distributed computing, custom GPTs, and robotics, which allows them to operate lean, cost-effective startups. These technical skills mean they can handle a significant portion of the development themselves, reducing reliance on external resources and allowing for faster prototyping and iterative product development.\n",
        "\n",
        "d. Access to Growing Funding Opportunities for AI and Robotics\n",
        "\n",
        "Venture capital and other funding sources are increasingly directed toward startups that leverage AI and robotics, as these areas are viewed as high-growth, high-impact fields. Certified engineers have expertise in trending fields such as multi-agent AI systems, AI ethics, and humanoid robotics, making their startups attractive to investors looking to fund cutting-edge AI applications and transformative robotics technologies.\n",
        "\n",
        "e. Built-In Flexibility to Adapt to Market Needs\n",
        "\n",
        "The certification’s breadth—including cloud-native deployment, knowledge graphs, API-as-a-Product, and custom AI agent creation—enables entrepreneurs to pivot based on market feedback. For instance, if there’s a demand for SaaS AI agents in customer service, founders can adapt their technical skills to meet this need, quickly creating and deploying new solutions that align with market trends.\n",
        "\n",
        "f. Scalability and Innovation Using AI-Driven Models\n",
        "\n",
        "With training in scalable cloud-native development and advanced knowledge of distributed computing tools like Ray and Kubernetes, certified engineers can build and grow their startups to serve a global customer base. They are equipped to create robust AI systems that can be easily scaled, attracting a larger client base and enhancing profitability.\n",
        "\n",
        "g. Lowered Barriers for Entry in GenAI and Robotics\n",
        "\n",
        "Traditionally, robotics and AI development were resource-intensive fields. However, cloud platforms, generative AI APIs, and containerized deployment options (skills certified engineers possess) significantly lower these barriers. This means startup founders can deploy cutting-edge AI without needing the massive infrastructure once required, allowing for quicker entry and competitive advantages in the market.\n",
        "\n",
        "h. Focus on Long-Term Industry Transformations\n",
        "\n",
        "With a solid grounding in industry applications, certified engineers understand how to leverage AI technologies to drive significant efficiencies, improve user experiences, and automate complex processes. This is particularly valuable in verticals like finance, healthcare, IoT, and cyber security, where AI-driven startups can introduce transformative solutions and redefine industry standards.\n",
        "\n",
        "In conclusion, Certified Agentic and Robotic AI Engineers have a rare combination of skills, technical flexibility, and industry insight that makes them well-suited for successful entrepreneurship. By leveraging their expertise in agentic and physical AI, cloud-native computing, and multi-agent systems, they are poised to create startups that meet the modern market’s demand for autonomous, intelligent, and scalable solutions. Their potential to succeed as founders is bolstered by both the technical depth of their skills and the strong demand for innovative AI applications ￼.\n",
        "\n",
        "Is the core program not too long, one and half years is a long time?\n",
        "The length of the program is 18 months which is broken down into six quarters of three months each. However, if students want to shorten the duration of the program they can take two or more courses simultaneously. The program covers a wide range of topics including Python, GenAI, Microservices, Database, Cloud Development, Fine-tuning, DevOps, GPTs, AI Agents, and Humanoids. The program is designed to give students a comprehensive understanding of Agentic AI and prepare them for careers in this field.\n",
        "\n",
        "Why don't we use TypeScript (Node.js) to develop APIs instead of using Python?\n",
        "We will not use Typescript in AI powered API development because Python is a priority with the AI community when working with AI and if any updates come in libraries they will first come for Python. Python is always a better choice when dealing with AI and API.\n",
        "\n",
        "Python is the de facto standard for AI Development.\n",
        "TypeScript is a more modern language that is gaining popularity for Web Development, but Python is more widely used and has a larger ecosystem of libraries and frameworks available, especially for AI.\n",
        "TypeScript is used for web user interfaces, while Python is used for APIs.\n",
        "Python is a more commonly used language for AI and API development, and it has a larger ecosystem of libraries and frameworks available for these purposes.\n",
        "TypeScript is a more modern language that is becoming increasingly popular for API development also, but it is still not as widely used as Python, especially for AI applications and development.\n",
        "\n",
        "What do we use LangGraph and LangChain for developing AI Agents as compared to other frameworks like CrewAI, AutoGen, or Litta?\n",
        "Let's break down the key features and differences of LangGraph, LangChain, CrewAI, and AutoGen:\n",
        "\n",
        "\n",
        "\n",
        "LangGraph\n",
        "Purpose: LangGraph is designed for building stateful, multi-actor applications with large language models (LLMs).\n",
        "\n",
        "Key Features:\n",
        "Cycles and Branching: Allows for loops and conditionals in applications.\n",
        "Persistence: Automatically saves state after each step, supporting error recovery and human-in-the-loop workflows.\n",
        "Integration Seamlessly integrates with LangChain and LangSmith, but can be used independently.\n",
        "Streaming Support: Outputs can be streamed as they are produced by each node.\n",
        " Human-in-the-Loop: Supports interrupting graph execution for approvals or edits.\n",
        "\n",
        "LangChain\n",
        "Purpose: LangChain is a framework for developing applications powered by LLMs.\n",
        "\n",
        "Key Features:\n",
        "Tool Integration: Allows chaining together LLM tasks.\n",
        "  Custom Tools: Developers can define custom tools for agents to use.\n",
        "  Memory Management: Supports conversational memory for multi-turn interactions.\n",
        " ReAct Framework: Provides a framework for chain-of-thought reasoning.\n",
        "\n",
        "CrewAI\n",
        "Purpose: CrewAI focuses on creating collaborative AI agents that can work together on tasks.\n",
        "\n",
        "Key Features:\n",
        "Collaboration: Designed for multi-agent collaboration and teamwork.\n",
        "Task Management: Efficiently manages tasks and workflows among agents.\n",
        "Scalability: Supports scaling up to handle complex, large-scale projects.\n",
        "\n",
        "AutoGen\n",
        "Purpose: AutoGen is aimed at automating the generation of code and other content.\n",
        "\n",
        "\n",
        "Key Features:\n",
        "Code Generation: Specialises in generating code based on prompts.\n",
        "Content Creation: Can automate the creation of various types of content.\n",
        "Customization: Allows for customization of the generated output.\n",
        "\n",
        "Letta\n",
        "Purpose: Letta focuses on adding memory to LLMs to enhance their\n",
        "reasoning capabilities and provide transparent long-term memory.\n",
        "Key Features:\n",
        "Memory Management: Supports advanced reasoning with long-term memory.\n",
        "Model Agnostic: Can work with any LLM, allowing developers to choose the best model for their use case.\n",
        "White Box Systems: Provides full visibility into the inner workings of LLMs and agents.\n",
        "Deployment: Offers both hosted cloud services and local deployment options.\n",
        "Use Cases: Ideal for personalised chatbots, agents connected to external data sources, and automated AI workflows.\n",
        "\n",
        "\n",
        "Each of these frameworks has its strengths and is suited for different types of AI agent development. LangGraph and LangChain are particularly strong in handling stateful, multi-actor applications and integrating with other tools, while CrewAI, AutoGen and Litta focus more on collaboration, automation, and long term memory respectively.\n",
        "\n",
        "\n",
        "Why don't we use Flask or Django for API development instead of FastAPI?\n",
        "FastAPI is a newer and more modern framework than Flask or Django. It is designed to be fast, efficient, and easy to use. FastAPI is also more scalable than Flask or Django, making it a better choice for large-scale projects.\n",
        "FastAPI is also more feature-rich than Flask or Django. It includes several built-in features that make it easy to develop APIs, such as routing, validation, and documentation.\n",
        "Overall, FastAPI is a better choice for API development than Flask or Django. It is faster, more scalable, and more feature-rich.\n",
        "\n",
        "\n",
        "Why do we need to learn Cloud technologies in a Generative AI program?\n",
        "Cloud technologies are essential for developing and deploying generative AI applications because they provide a scalable and reliable platform for hosting and managing complex workloads.\n",
        "\n",
        "Cloud computing offers a vast pool of resources that can be provisioned on demand, which is ideal for generative AI applications that can be computationally intensive.\n",
        "Cloud providers offer a wide range of services that can be used to support generative AI applications, including storage, computing, networking, and machine learning.\n",
        "Cloud services are typically more cost-effective than on-premises infrastructure, which can be a significant advantage for generative AI applications that are often used for large-scale projects.\n",
        "\n",
        "The Certified Agentic and Robotic AI Engineering Program teaches you how to use cloud native services, including containers and Kubernetes, to deploy your applications to the cloud. You will also learn how to use Docker containers to package and deploy your applications, and how to use Terraform to manage your cloud infrastructure.\n",
        "\n",
        "By the end of the program, you will be able to:\n",
        "Use Docker containers to package and deploy your applications\n",
        "Develop and deploy generative AI applications to the cloud\n",
        "Manage your cloud infrastructure using Terraform\n",
        "\n",
        "\n",
        "What is the purpose of Docker Containers and what are the benefits of deploying them with Docker Compose, and Kubernetes?\n",
        "Docker Containers are a way to package software into a single unit that can be run on any machine, regardless of its operating system. It is used to create a Dockerfile, which is a text file that describes how to build a Docker image. The image is then used to create a container, which is a running instance of the image. This makes them ideal for deploying applications on a variety of platforms, including cloud-based services.\n",
        "Docker Compose is a tool provided by Docker that allows you to define and manage multi-container Docker applications locally. It enables you to use a YAML file to configure the services, networks, and volumes needed for your application's setup. With Docker Compose, you can describe the services your application requires, their configurations, dependencies, and how they should interact with each other, all in a single file. This makes it easier to orchestrate complex applications locally composed of multiple interconnected containers.\n",
        "Kubernetes is a container orchestration system that automates the deployment, scaling, and management of containerized applications. It allows you to run multiple containers on a single machine or across multiple machines. It is an open source and can be deployed in your data centre or the cloud.\n",
        "\n",
        "\n",
        "What is the purpose of learning to develop APIs in a Generative AI program?\n",
        "APIs (Application Programming Interfaces) are used to connect different software applications and services together. They are the building blocks of the internet and are essential for the exchange of data between different systems.\n",
        "\n",
        "In the Certified Agentic and Robotic AI Engineering Program, students will learn to develop APIs not just as a backend but also as a product itself. In this model, the API is at the core of the business's value.\n",
        "\n",
        "APIs are used to make it possible for different software applications to communicate with each other.\n",
        "APIs are used to access data from a remote server.\n",
        "APIs are used to create new services or applications that are integrated with existing systems.\n",
        "APIs are used to improve the security of applications by providing a way to control access to data.\n",
        "By learning to develop APIs, students will gain the skills necessary to create powerful and efficient software applications that can be used to solve a variety of business problems.\n",
        "\n",
        "\n",
        "What is the purpose of using Python-based FastAPI and related technologies?\n",
        "In this Program, students will learn how to use Python-based FastAPI as a core library for API development.\n",
        "\n",
        "FastAPI is a high-performance, lightweight, and easy-to-use framework for building APIs.\n",
        "It is designed to be fast, scalable, and secure.\n",
        "FastAPI is compatible with a wide range of programming languages and frameworks, making it a good choice for developers with different skill sets.\n",
        "Students will also learn about the following related technologies:\n",
        "Pydantic: Pydantic is a Python library that helps to improve the quality of your code by checking for errors and potential problems.\n",
        "GQL: The Graph Query Language (GQL) is an international standard published by the International Organization for Standardization (ISO) as ISO/IEC 39075:2024. GQL is designed for querying property graphs and is the first database query language ISO has published since SQL in 1987. It defines data structures and operations for creating, accessing, querying, maintaining, and controlling property graphs, providing a standardised way to manage graph data across different implementations.\n",
        "Neo4j: Neo4j is a powerful, open-source graph database designed to store and manage highly connected data. Unlike traditional relational databases, Neo4j uses a graph model, where data is represented as nodes, relationships, and properties. This structure is ideal for applications where relationships between data points are as important as the data itself.\n",
        "\n",
        "By the end of the program, students will be able to use Python-based FastAPI to develop APIs that are fast, scalable, and secure.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "What does the API-as-a-Product model entail?\n",
        "API-as-a-Product is a type of Software-as-a-Service that monetizes niche functionality, typically served over HTTP. In this model, the API is at the core of the business's value. The API-as-a-Product model is different from the traditional API model, where APIs are used as a means to access data or functionality from another application. In the API-as-a-Product model, the API itself is the product that is being sold.\n",
        "\n",
        "The benefits of the API-as-a-Product model include:\n",
        "\n",
        "Increased flexibility: APIs can be used to access data or functionality from any application, regardless of the underlying platform or technology. This gives businesses greater flexibility in how they integrate APIs into their applications.\n",
        "Reduced development costs: APIs can be reused by multiple applications, which can save businesses the time and expense of developing their custom APIs.\n",
        "Improved scalability: APIs can be scaled up or down as needed, which makes them well-suited for businesses with fluctuating or unpredictable traffic demands.\n",
        "Enhanced security: APIs can be more secure than traditional methods of data exchange, as they can be protected by a variety of security measures, such as encryption and access control.\n",
        "\n",
        "\n",
        "\n",
        "What are the benefits of using Docker Containers for development, testing, and deployment?\n",
        "Docker Containers are a fundamental building block for development, testing, and deployment because they provide a consistent environment that can be used across different systems. This eliminates the need to worry about dependencies or compatibility issues, and it can help to improve the efficiency of the development process. Additionally, Docker Containers can be used to isolate applications, which can help to improve security and make it easier to manage deployments.\n",
        "\n",
        "\n",
        "What is the advantage of using open Docker, Kubernetes, and Terraform technologies instead of using AWS, Azure, or Google Cloud technologies?\n",
        "Using open-source technologies like Docker, Kubernetes, and Terraform offers several advantages over relying solely on proprietary cloud services from AWS, Azure, or Google Cloud. Here’s a detailed comparison:\n",
        "\n",
        "Advantages of Using Docker, Kubernetes, and Terraform (Open Technologies)\n",
        "\n",
        "1. Portability and Flexibility:\n",
        "   - Vendor Agnostic: These tools are cloud-agnostic, meaning you can run your applications on any cloud provider or on-premises infrastructure without being locked into a specific vendor.\n",
        "   - Ease of Migration: Applications packaged in Docker containers can easily be moved across different environments, and Kubernetes provides a consistent orchestration layer, ensuring seamless transitions.\n",
        "\n",
        "2. Cost Efficiency:\n",
        "   - Avoid Vendor Lock-In: Being locked into a single cloud provider can lead to higher costs over time. Using open technologies allows you to leverage competitive pricing from multiple providers or even use on-premises resources.\n",
        "   - Optimised Resource Utilisation: Kubernetes helps in efficiently managing resources through automated scaling and load balancing, potentially reducing costs.\n",
        "\n",
        "3. Community and Ecosystem:\n",
        "   - Open Source: These tools are backed by strong, active open-source communities that continuously improve the software, provide support, and share best practices.\n",
        "   - Ecosystem: A rich ecosystem of tools and integrations is available, providing flexibility to choose the best components that fit your specific needs.\n",
        "\n",
        "4. Standardisation and Consistency:\n",
        "   - Unified Platform: Using Docker for containerization, Kubernetes for orchestration, and Terraform for infrastructure as code (IaC) provides a standardised way to deploy, manage, and scale applications across different environments.\n",
        "   - Consistency Across Environments: These tools ensure that your development, staging, and production environments are consistent, reducing bugs and deployment issues.\n",
        "\n",
        "5. Customization and Control:\n",
        "   - Full Control: Open-source tools give you complete control over your infrastructure and deployment pipelines. You can customise and extend the functionality to suit specific requirements.\n",
        "   - Transparency: Access to the source code means you can audit and modify the software to meet your security and compliance needs.\n",
        "\n",
        "Advantages of Using AWS, Azure, or Google Cloud Technologies\n",
        "\n",
        "1. Managed Services:\n",
        "   - **Ease of Use:** Cloud providers offer a wide range of managed services that abstract away the complexity of setting up and managing infrastructure. This can save time and reduce operational overhead.\n",
        "   - Integrated Solutions: These platforms provide integrated services and tools, such as databases, machine learning, analytics, and monitoring, which can be easily combined to build complex applications.\n",
        "\n",
        "2. Scalability and Reliability:\n",
        "   - Global Infrastructure: Cloud providers have extensive global infrastructure, ensuring high availability, redundancy, and low latency.\n",
        "   - Auto-Scaling: Advanced auto-scaling capabilities can dynamically adjust resources to meet changing demands, ensuring optimal performance.\n",
        "\n",
        "3. Security and Compliance:\n",
        "   - Built-In Security: Cloud providers offer robust security features, including identity and access management, encryption, and compliance certifications, helping to protect your data and meet regulatory requirements.\n",
        "   - Automatic Updates: Managed services often include automatic updates and patches, reducing the risk of security vulnerabilities.\n",
        "\n",
        "4. Innovation and Support:\n",
        "   - Cutting-Edge Technology: Major cloud providers continuously innovate and introduce new services and features, allowing you to leverage the latest technologies without significant investment.\n",
        "   - Support and SLA: Comprehensive support services and Service Level Agreements (SLAs) ensure that you have access to expert help and guaranteed uptime.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Choosing between open-source technologies like Docker, Kubernetes, and Terraform versus proprietary cloud services from AWS, Azure, or Google Cloud depends on your specific needs and priorities.\n",
        "\n",
        "- Open Technologies: Offer portability, cost efficiency, customization, and control, making them ideal for multi-cloud strategies, avoiding vendor lock-in, and having more control over your infrastructure.\n",
        "- Cloud Providers: Provide ease of use, managed services, scalability, security, and access to cutting-edge technology, which can be advantageous for rapid development, scaling, and leveraging advanced services.\n",
        "\n",
        "In many cases, a hybrid approach that combines the strengths of both open-source tools and cloud provider services can provide the best of both worlds, allowing you to optimise for cost, flexibility, and innovation.\n",
        "\n",
        "Why in this program are we not learning to build LLMs ourselves? How difficult is it to develop an LLM like ChatGPT 4 or Google’s Gemini?\n",
        "Developing an LLM like ChatGPT 4 or Google Gemini is extremely difficult and requires a complex combination of resources, expertise, and infrastructure. Here's a breakdown of the key challenges:\n",
        "\n",
        "Technical hurdles:\n",
        "\n",
        "Massive data requirements: Training these models requires an immense amount of high-quality data, often exceeding petabytes. Compiling, cleaning, and structuring this data is a monumental task.\n",
        "Computational power: Training LLMs demands incredible computational resources, like high-performance GPUs and specialised AI hardware. Access to these resources and the ability to optimise training processes are crucial.\n",
        "Model architecture: Designing the LLM's architecture involves complex decisions about parameters, layers, and attention mechanisms. Optimising this architecture for performance and efficiency is critical.\n",
        "Evaluation and bias: Evaluating the performance of LLMs involves diverse benchmarks and careful monitoring for biases and harmful outputs. Mitigating these biases is an ongoing research challenge.\n",
        "\n",
        "Resource and expertise:\n",
        "\n",
        "Team effort: Developing an LLM like ChatGPT 4 or Google Gemini requires a large team of experts across various disciplines, including AI researchers, machine learning engineers, data scientists, and software developers.\n",
        "Financial investment: The financial resources needed are substantial, covering costs for data acquisition, hardware, software, and talent. Access to sustained funding is critical.\n",
        "\n",
        "\n",
        "Additionally:\n",
        "\n",
        "Ethical considerations: LLMs raise ethical concerns like potential misuse, misinformation, and societal impacts. Responsible development and deployment are crucial.\n",
        "Rapidly evolving field: The LLM landscape is constantly evolving, with new research, models, and benchmarks emerging. Staying abreast of these advancements is essential.\n",
        "\n",
        "Therefore, while ChatGPT 4 and Google Gemini have made impressive strides, developing similar LLMs remains a daunting task accessible only to a handful of organisations with the necessary resources and expertise.\n",
        "\n",
        "In simpler terms, it's like building a skyscraper of knowledge and intelligence. You need the right materials (data), the right tools (hardware and software), the right architects (experts), and a lot of hard work and attention to detail to make it stand tall and function flawlessly.\n",
        "\n",
        "Developing similar models would be a daunting task for individual developers or smaller teams due to the enormous scale of resources and expertise needed. However, as technology progresses and research findings become more accessible, it might become incrementally more feasible for a broader range of organisations or researchers to work on similar models, albeit at a smaller scale or with fewer resources. At that time we might also start to focus on developing LLMs ourselves.\n",
        "\n",
        "To sum up, the focus of the program is not on LLM model development but on applied Cloud GenAI Engineering (GenEng), application development, and fine-tuning of foundational models. The program covers a wide range of topics including Python, GenAI, Microserices, API, Database, Cloud Development, and DevOps, which will give students a comprehensive understanding of generative AI and prepare them for careers in this field.\n",
        "\n",
        "\n",
        "Business wise does it make more sense to develop LLMs ourselves from scratch or use LLMs developed by others and build applications using these tools by using APIs and/or fine-tuning them?\n",
        "Whether it makes more business sense to develop LLMs from scratch or leverage existing ones through APIs and fine-tuning depends on several factors specific to your situation. Here's a breakdown of the pros and cons to help you decide:\n",
        "\n",
        "Developing LLMs from scratch:\n",
        "\n",
        "Pros:\n",
        "\n",
        "Customization: You can tailor the LLM to your specific needs and data, potentially achieving higher performance on relevant tasks.\n",
        "Intellectual property: Owning the LLM allows you to claim intellectual property rights and potentially monetize it through licensing or other means.\n",
        "Control: You have full control over the training data, algorithms, and biases, ensuring alignment with your ethical and business values.\n",
        "\n",
        "Cons:\n",
        "\n",
        "High cost: Building and training LLMs require significant technical expertise, computational resources, and data, translating to high financial investment.\n",
        "Time commitment: Developing an LLM is a time-consuming process, potentially delaying your go-to-market with your application.\n",
        "Technical expertise: You need a team of highly skilled AI specialists to design, train, and maintain the LLM.\n",
        "\n",
        "Using existing LLMs:\n",
        "\n",
        "Pros:\n",
        "\n",
        "Lower cost: Leveraging existing LLMs through APIs or fine-tuning is significantly cheaper than building them from scratch.\n",
        "Faster time to market: You can quickly integrate existing LLMs into your applications, accelerating your launch timeline.\n",
        "Reduced technical burden: You don't need a large team of AI specialists to maintain the LLM itself\n",
        "\n",
        "Cons:\n",
        "\n",
        "Less customization: Existing LLMs are not specifically designed for your needs, potentially leading to lower performance on some tasks.\n",
        "Limited control: You rely on the data and biases of the existing LLM, which might not align with your specific requirements.\n",
        "Dependency on external parties: You are dependent on the availability and maintenance of the LLM by its developers.\n",
        "\n",
        "\n",
        "Here are some additional factors to consider:\n",
        "\n",
        "The complexity of your application: Simpler applications might benefit more from existing LLMs, while highly complex ones might require the customization of a dedicated LLM.\n",
        "Your available resources: If you have the financial and technical resources, developing your own LLM might be feasible. Otherwise, existing options might be more practical.\n",
        "Your competitive landscape: If your competitors are using LLMs, you might need to follow suit to remain competitive.\n",
        "Ultimately, the best decision depends on your specific needs, resources, and business goals. Carefully evaluating the pros and cons of each approach will help you choose the strategy that best aligns with your success.\n",
        "\n",
        "What are Custom GPTs?\n",
        "\"Custom GPTs\" refers to specialised versions of the Generative Pre-trained\n",
        "Transformer (GPT) models that are tailored for specific tasks, industries, or data types. These custom models are adapted from the base GPT architecture, which is a type of language model developed by OpenAI. Custom GPT models are trained or fine-tuned on specific datasets or for particular applications, allowing them to perform better in those contexts compared to the general-purpose models.\n",
        "\n",
        "Here are some examples of what custom GPT models might be used for:\n",
        "\n",
        "1. Industry-Specific Needs: A custom GPT for legal, medical, or financial industries could be trained on domain-specific texts to understand and generate industry-specific language more accurately.\n",
        "\n",
        "2. Language and Localization: Models can be customised for different languages or dialects that might not be well-represented in the training data of the base model.\n",
        "\n",
        "3. Company-Specific Applications: Organisations might develop a custom GPT model trained on their own documents and communications to assist with internal tasks like drafting emails, generating reports, or providing customer support.\n",
        "\n",
        "4. Educational Purposes: Educational institutions might develop custom GPTs trained on educational material to assist in creating teaching materials or providing tutoring in specific subjects.\n",
        "\n",
        "5. Creative Writing and Entertainment: Custom models could be trained on specific genres of literature or scripts to assist in creative writing or content creation.\n",
        "\n",
        "6. Technical and Scientific Research: A custom GPT model could be trained on scientific literature to assist researchers in summarising papers, generating hypotheses, or even drafting new research.\n",
        "\n",
        "These custom models are created through a process of fine-tuning, where the base GPT model is further trained (or 'fine-tuned') on a specific dataset. This process allows the model to become more adept at understanding and generating text that is relevant to the specific use case. Fine-tuning requires expertise in machine learning and natural language processing, as well as access to relevant training data.\n",
        "\n",
        "What are Actions in GPTs?\n",
        "Actions are a way to connect custom GPTs to external APIs, allowing them to access data or interact with the real-world. For example, you can use actions to create a GPT that can book flights, send emails, or order pizza. Actions are defined using the OpenAPI specification, which is a standard for describing APIs. You can import an existing OpenAPI specification or create a new one using the GPT editor.\n",
        "\n",
        "What are AI Agents and how do they differ from Custom GPTs?\n",
        "AI Agents and Custom GPTs are both tools that utilise artificial intelligence to perform tasks, but they have distinct functionalities and use cases. Here’s a breakdown of their differences:\n",
        "\n",
        "AI Agents\n",
        "AI Agents are autonomous programs that can perceive their environment, make decisions, and act upon them to achieve specific goals. They often interact with other systems or users, continuously learning and adapting based on their experiences.\n",
        "\n",
        "Key Characteristics:\n",
        "1. Autonomy: AI Agents operate independently without continuous human intervention.\n",
        "2. Learning: They often employ machine learning algorithms to improve performance over time.\n",
        "3. Interactivity: AI Agents can interact with their environment, other systems, and users.\n",
        "4. Goal-Oriented: They are designed to achieve specific objectives and can adapt their actions to optimise towards these goals.\n",
        "5. Multi-Modal Capabilities: AI Agents can incorporate various forms of AI, such as computer vision, natural language processing, and decision-making algorithms.\n",
        "\n",
        "Examples:\n",
        "- Robotics: Autonomous robots that navigate and perform tasks.\n",
        "- Virtual Assistants: Programs like Siri or Alexa that interact with users and perform tasks based on voice commands.\n",
        "- Game AI: Non-player characters (NPCs) that adapt and react to player actions.\n",
        "\n",
        "Custom GPTs\n",
        "Custom GPTs are tailored instances of OpenAI’s ChatGPT, launched in late 2022. They are designed for specific purposes and enhanced with context.\n",
        "Each custom GPT can have a unique “personality,” including tone of voice, language complexity, and responsiveness to specific topics. For example, a financial institution’s custom GPT could be trained on financial reports and industry-specific terminology, while a healthcare provider’s version might focus on medical literature and health policy documents\n",
        "\n",
        "\n",
        "Key Differences\n",
        "\n",
        "1. Autonomy:\n",
        "   - AI Agents: Operate autonomously and continuously interact with their environment.\n",
        "   - Custom GPTs: Typically respond to specific inputs and generate outputs accordingly, but don’t operate autonomously beyond text generation tasks.\n",
        "\n",
        "2. Learning and Adaptation:\n",
        "   - AI Agents: Often incorporate continuous learning and adaptation mechanisms.\n",
        "   - Custom GPTs: Rely on pre-training and fine-tuning phases, with limited continuous learning capabilities.\n",
        "\n",
        "4. Interactivity:\n",
        "   - AI Agents: Can interact with both digital and physical environments.\n",
        "   - Custom GPTs: Primarily interact through text-based inputs and outputs.\n",
        "\n",
        "In summary, while both AI Agents and Custom GPTs utilise AI, AI Agents are designed for autonomous, goal-oriented actions in diverse environments, and Custom GPTs are specialised in generating and understanding human-like text for specific applications.\n",
        "\n",
        "Do we need to use Design Thinking and BDD for designing custom GPTs and AI Agents?\n",
        "Design Thinking and Behavior-Driven Development (BDD) are methodologies that can greatly enhance the process of designing custom GPTs and AI Agents, though they are not strictly necessary. Here’s how each can be beneficial:\n",
        "\n",
        "Design Thinking\n",
        "Design Thinking is a user-centred approach to innovation and problem-solving that involves understanding the user, challenging assumptions, redefining problems, and creating innovative solutions through iterative prototyping and testing.\n",
        "\n",
        "Benefits for Custom GPTs and AI Agents:\n",
        "1. User-Centric Focus: Ensures that the AI solutions are tailored to the actual needs and pain points of users.\n",
        "2. Empathy: Helps in understanding the context and environment in which the AI will be used, leading to more relevant and effective solutions.\n",
        "3. Iterative Development: Encourages continuous testing and refinement of ideas, leading to more robust and user-friendly AI models.\n",
        "4. Collaboration: Promotes cross-disciplinary collaboration, which can bring diverse perspectives and expertise to the design process.\n",
        "\n",
        "\n",
        "Behaviour-Driven Development (BDD)\n",
        "BDD is a software development methodology that encourages collaboration between developers, QA, and non-technical stakeholders through the use of natural language descriptions of the desired behaviour of the software.\n",
        "\n",
        "Benefits for Custom GPTs and AI Agents:\n",
        "1. Clear Requirements: Ensures that the requirements are clearly understood and agreed upon by all stakeholders.\n",
        "2. Testable Scenarios: Facilitates the creation of testable scenarios that can validate the AI’s behaviour against the expected outcomes.\n",
        "3. Documentation: Provides clear and comprehensive documentation of the AI’s intended behaviour, which is useful for future maintenance and enhancements.\n",
        "4. Alignment: Ensures that the development stays aligned with business goals and user expectations.\n",
        "\n",
        "Application in Designing Custom GPTs and AI Agents\n",
        "\n",
        "For Custom GPTs:\n",
        "- Design Thinking:\n",
        "  - Understand the specific use cases and user interactions where the GPT will be applied.\n",
        "  - Iterate on the model’s performance by gathering user feedback and refining the fine-tuning process.\n",
        "  - Prototype different conversation flows and evaluate their effectiveness with real users.\n",
        "\n",
        "- BDD:\n",
        "  - Define the expected behaviours of the GPT in natural language scenarios.\n",
        "  - Create automated tests that validate the GPT’s responses against these scenarios.\n",
        "  - Ensure that the GPT’s behaviour aligns with user stories and business requirements.\n",
        "\n",
        "For AI Agents:\n",
        "- Design Thinking:\n",
        "  - Map out the user journey and identify critical interaction points where the AI Agent will provide value.\n",
        "  - Prototype and test the agent’s interactions in various environments to ensure robustness and usability.\n",
        "  - Use empathy maps and personas to better understand and anticipate user needs and behaviours.\n",
        "\n",
        "- BDD:\n",
        "  - Write behaviour scenarios that describe how the AI Agent should react in different situations.\n",
        "  - Develop tests that simulate these scenarios to verify the agent’s decision-making and learning processes.\n",
        "  - Continuously refine the agent’s behaviour based on test results and user feedback.\n",
        "\n",
        "While not strictly necessary, Design Thinking and BDD can significantly enhance the design and development process of custom GPTs and AI Agents by ensuring a user-centred approach, clear requirements, and continuous improvement through iterative testing and feedback. These methodologies help in creating more effective, reliable, and user-friendly AI solutions.\n",
        "\n",
        "\n",
        "\n",
        "What is Ray, how do we use it in developing AI?\n",
        "Ray is an open-source distributed computing framework specifically designed to simplify the development and scaling of AI applications and other data-intensive tasks. It provides a flexible platform for building distributed applications that can scale from a single machine to a large cluster, making it ideal for AI workloads that demand substantial computational power, such as machine learning training, reinforcement learning, data processing, and model serving.\n",
        "\n",
        "Here’s how Ray is used in AI development and why it’s advantageous:\n",
        "\n",
        "a. Distributed Computing for AI Workloads\n",
        "\n",
        "Ray’s primary feature is its ability to distribute computation across multiple CPUs and GPUs, allowing developers to run tasks in parallel and manage complex workflows efficiently. This is particularly beneficial in AI development, where training large models or processing vast datasets can be time-consuming and resource-intensive.\n",
        "Parallelization: Ray enables developers to parallelize tasks, such as data preprocessing or model training, across multiple nodes, reducing runtime and maximising resource usage.\n",
        "Scaling: AI tasks can be distributed across clusters, allowing them to scale up to cloud-based environments or large on-premises setups with minimal code changes.\n",
        "\n",
        "b. Simplified Development with Python API\n",
        "Ray is designed to work seamlessly with Python, which is the most popular language in AI and machine learning. Its intuitive API makes it easy for developers to turn existing Python functions into distributed tasks, which reduces the complexity of parallel programming.\n",
        "Ease of Use: Developers can use familiar Python code without deep knowledge of distributed systems, thanks to Ray’s ability to automatically handle task scheduling, data sharing, and fault tolerance.\n",
        "Flexible Abstractions: Ray provides high-level abstractions, such as tasks and actors, to manage distributed computations efficiently. This flexibility allows developers to easily implement complex parallel workflows.\n",
        "\n",
        "c. Frameworks Built on Ray for Specialized AI Tasks\n",
        "Ray includes several libraries tailored for specific AI use cases, simplifying complex tasks and streamlining workflow integration:\n",
        "Ray Tune: Used for hyperparameter tuning, which is crucial in optimizing AI models. Ray Tune distributes the tuning process, allowing multiple hyperparameter configurations to be evaluated in parallel, which speeds up model optimization significantly.\n",
        "Ray RLlib: A library for reinforcement learning (RL) that simplifies the implementation and scaling of RL algorithms. It supports a range of algorithms out of the box and can run on single machines or distributed clusters, making it suitable for large-scale RL projects.\n",
        "Ray Serve: Designed for model serving, it provides a scalable, flexible way to deploy trained AI models as services that can handle multiple requests in real-time, making it ideal for production applications.\n",
        "\n",
        "d. Use Cases in AI Development\n",
        "Ray is applicable across various stages of AI development, including:\n",
        "Data Processing and Preprocessing: Ray can distribute data loading, transformation, and cleaning tasks, which are often bottlenecks in AI workflows, across a cluster. This accelerates the process and allows for more efficient handling of large datasets.\n",
        "Model Training: With Ray, model training can be distributed across multiple GPUs or nodes, reducing training time and enabling the handling of larger models or datasets.\n",
        "Hyperparameter Optimization: Ray Tune simplifies running multiple experiments to find the best model configurations, automating the tuning process and improving model performance.\n",
        "Reinforcement Learning: Ray RLlib supports scaling RL algorithms, which are often computationally intensive and require significant parallelization to achieve optimal performance.\n",
        "Model Deployment: Ray Serve allows developers to deploy models for real-time predictions, handling requests with high throughput and low latency.\n",
        "\n",
        "5. Advantages of Using Ray in AI\n",
        "Scalability: Ray’s distributed architecture makes it easy to scale AI workloads, whether they are running on a laptop, a multi-GPU setup, or a cloud cluster.\n",
        "Efficiency: By enabling distributed processing and parallelism, Ray reduces the computational time needed for data processing, model training, and serving, leading to faster iteration cycles.\n",
        "Unified Framework: Ray integrates various stages of the AI pipeline (training, tuning, deployment) under a single platform, making it easier for teams to manage and scale AI workflows consistently.\n",
        "Cost Optimization: By optimising resource usage across clusters, Ray helps minimise infrastructure costs, especially in cloud environments where resources are billed based on usage.\n",
        "\n",
        "\n",
        "\n",
        "When Fine-Tuning Open-Source Large Language Models, how is Ray and PyTorch important and what role do these libraries play?\n",
        "Fine-tuning open-source Large Language Models (LLMs) like Meta’s LLaMA or other foundational models is a complex and resource-intensive task that involves adapting a pre-trained model to perform better on specific tasks or domains. Ray and PyTorch play essential roles in this process by providing tools to efficiently handle the heavy computational requirements of fine-tuning, making it scalable, flexible, and streamlined.\n",
        "\n",
        "Here’s how each of these libraries contributes to fine-tuning LLMs:\n",
        "\n",
        "a. Role of PyTorch in Fine-Tuning LLMs\n",
        "PyTorch is a deep learning framework widely used for developing and fine-tuning neural networks, including LLMs. It provides the foundational tools for setting up, training, and optimising neural network models.\n",
        "Flexible Model Building: PyTorch’s dynamic computation graph, also known as define-by-run, allows researchers and developers to easily customise the LLM architecture or adjust training configurations to fit specific tasks. This is especially useful when experimenting with different model architectures or training techniques.\n",
        "Pre-Trained Model Integration: PyTorch integrates seamlessly with the Hugging Face Transformers library, which provides access to many pre-trained LLMs. This integration simplifies the fine-tuning process, allowing developers to load pre-trained models, prepare them for task-specific data, and modify them as needed.\n",
        "GPU Acceleration: PyTorch supports GPU and distributed training, crucial for handling the immense computational requirements of fine-tuning LLMs. Large language models have millions to billions of parameters, so GPU acceleration dramatically reduces training time.\n",
        "Tools for Optimization: PyTorch offers numerous built-in optimizers and training utilities, such as AdamW (an optimizer widely used in NLP), learning rate schedulers, and gradient clipping. These tools help ensure that fine-tuning is efficient, effective, and stable.\n",
        "\n",
        "b. Role of Ray in Scaling and Distributing the Fine-Tuning Process\n",
        "Ray is critical for scaling the fine-tuning process across multiple GPUs or even across distributed clusters, which is essential for handling the large-scale computations required for LLMs.\n",
        "Distributed Training: Ray makes it easy to distribute PyTorch workloads across multiple CPUs or GPUs by turning Python functions into remote functions that can be executed concurrently. This is useful for model parallelism, where different parts of the model are processed on different devices, or for data parallelism, where the same model is trained on different data batches across multiple devices.\n",
        "Hyperparameter Tuning with Ray Tune: Fine-tuning often involves adjusting hyperparameters like learning rate, batch size, or dropout rates. Ray Tune, a library within Ray, automates this process by running multiple configurations simultaneously, speeding up the search for the optimal settings.\n",
        "Efficient Resource Utilisation: Ray helps manage computational resources efficiently by automatically handling task scheduling, load balancing, and fault tolerance. This allows developers to focus on model training without manually managing cluster infrastructure.\n",
        "Integration with PyTorch Distributed: Ray seamlessly integrates with PyTorch’s native distributed framework (such as torch.distributed). This allows developers to leverage Ray’s parallelism tools along with PyTorch’s model training capabilities to create efficient distributed training pipelines.\n",
        "\n",
        "c. Combined Workflow Using PyTorch and Ray for Fine-Tuning\n",
        "The ideal workflow for fine-tuning LLMs combines the strengths of both PyTorch and Ray:\n",
        "Model Loading and Preparation: Load the pre-trained model in PyTorch and set up training parameters.\n",
        "Data Parallelism with Ray: Distribute data across multiple GPUs or nodes using Ray to increase training speed.\n",
        "Hyperparameter Tuning with Ray Tune: Use Ray Tune to test various configurations, automatically tracking results and selecting the optimal combination.\n",
        "Training Management and Optimization: Use PyTorch for efficient training with distributed backpropagation and GPU utilisation, while Ray manages the orchestration across clusters.\n",
        "\n",
        "d. Advantages of Using Ray and PyTorch Together in Fine-Tuning\n",
        "Scalability: Ray allows PyTorch models to scale beyond a single device or server, which is crucial for handling large-scale models and datasets.\n",
        "Speed and Efficiency: Distributing data processing and training across multiple devices can significantly reduce fine-tuning time, making iterative experimentation faster.\n",
        "Flexibility: Ray’s integration with Python and PyTorch allows developers to build complex distributed training pipelines without needing to learn complex distributed systems programming.\n",
        "Cost Efficiency: By efficiently managing hardware resources, Ray helps reduce infrastructure costs, particularly in cloud environments where resources are billed based on usage.\n",
        "\n",
        "Summary\n",
        "Ray and PyTorch together provide a robust platform for fine-tuning large language models. PyTorch offers the deep learning capabilities required for LLM training, while Ray adds scalability and resource management, making distributed fine-tuning accessible, efficient, and effective. This combination enables developers to fine-tune powerful models on large datasets across distributed environments, paving the way for faster, scalable AI development.\n",
        "\n",
        "\n",
        "\n",
        "In this course PyTorch plays a crucial role in the fine-tuning of open-source LLMs, why don't we use TensorFlow instead?\n",
        "While TensorFlow is a powerful and widely-used deep learning framework, PyTorch offers several advantages that make them particularly well-suited for fine-tuning open-source Large Language Models (LLMs). Here’s a detailed comparison highlighting why PyTorch might be preferred over TensorFlow for this specific task:\n",
        "\n",
        "PyTorch vs. TensorFlow\n",
        "\n",
        "a. Dynamic Computation Graphs:\n",
        "   - PyTorch: Uses dynamic computation graphs (define-by-run), which allow for greater flexibility and ease of debugging. This is especially useful when experimenting with new models and training strategies.\n",
        "   - TensorFlow: Initially used static computation graphs (define-and-run). Although TensorFlow 2.0 introduced eager execution to support dynamic graphs, PyTorch's implementation is often considered more intuitive and easier to work with for dynamic tasks.\n",
        "\n",
        "b. Ease of Use:\n",
        "   - PyTorch: Known for its simplicity and clear, Pythonic code, which makes it easier to learn and use, especially for research and prototyping.\n",
        "   - TensorFlow: While TensorFlow 2.0 improved usability, it is still considered more complex compared to PyTorch, particularly for newcomers.\n",
        "\n",
        "c. Community and Ecosystem:\n",
        "   - PyTorch: Has seen rapid adoption in the research community, leading to a rich ecosystem of tools, libraries, and community support. Libraries like Hugging Face’s Transformers are built primarily for PyTorch, offering extensive support for LLMs.\n",
        "   - TensorFlow: Has a strong industrial presence and is widely used in production environments. However, the research community has increasingly favoured PyTorch.\n",
        "\n",
        "d. Integration with Hugging Face:\n",
        "   - PyTorch: Hugging Face’s Transformers library, which is a go-to for working with LLMs, is deeply integrated with PyTorch. This library provides pre-trained models, tokenizers, and utilities that simplify the process of fine-tuning LLMs.\n",
        "   - TensorFlow: Although Hugging Face provides TensorFlow support, the integration is not as seamless or feature-rich as it is with PyTorch.\n",
        "\n",
        "\n",
        "While TensorFlow remains a powerful framework, particularly in production environments, PyTorch and provides a combination of flexibility, ease of use, and community support that make them particularly well-suited for the fine-tuning of open-source LLMs.\n",
        "\n",
        "\n",
        "\n",
        "What is Physical AI?\n",
        "Physical AI refers to the integration of artificial intelligence with physical entities, such as robots, that can operate and interact in the real world. This concept involves AI systems that not only process data and make decisions but also perform physical actions and understand the laws of physics.\n",
        "\n",
        "Key Characteristics:\n",
        "\n",
        "1. Real-World Interaction:\n",
        "   - Physical AI systems can perceive their environment through sensors, process this information, and take appropriate actions using actuators.\n",
        "\n",
        "2. Embodiment:\n",
        "   - Unlike purely digital AI, Physical AI involves AI embedded in physical bodies, like humanoid robots, which can navigate and manipulate the physical world.\n",
        "\n",
        "3. Understanding Physics:\n",
        "   - These AI systems are designed to comprehend and adhere to the physical laws that govern real-world interactions, such as gravity, friction, and object dynamics.\n",
        "\n",
        "4. Human-like Functionality:\n",
        "   - Humanoid robots are a prime example of Physical AI, as they are built to perform tasks in environments designed for humans, utilising a form factor that mirrors human anatomy.\n",
        "\n",
        "5. Data-Driven Training:\n",
        "   - Physical AI leverages vast amounts of real-world data to train AI models, enabling robots to improve their performance through machine learning and interaction experiences.\n",
        "\n",
        "Applications:\n",
        "- Healthcare:\n",
        "  - Assistive robots that help with patient care, rehabilitation, and surgery.\n",
        "- Service Industry:\n",
        "  - Robots that perform tasks such as cleaning, delivery, and customer service.\n",
        "- Manufacturing:\n",
        "  - Industrial robots that assemble products, manage inventory, and ensure quality control.\n",
        "- Exploration:\n",
        "  - Robots designed for exploration in environments like space, underwater, or disaster zones.\n",
        "\n",
        "Physical AI represents a significant shift from traditional AI applications confined to virtual environments. It aims to bridge the gap between digital intelligence and physical capability, creating systems that can understand and interact with the world in a human-like manner. This evolution has the potential to revolutionise various industries by enhancing automation, improving efficiency, and enabling new forms of human-machine collaboration.\n",
        "\n",
        "\n",
        "What are the different specialisations offered at the end of the program and what are their benefits?\n",
        "At the end of the certification program we offer eight specialisations in different fields:\n",
        "\n",
        "AI-701: Healthcare and Medical Agentic AI: This specialisation will teach students how to use agentic and generative AI to improve healthcare and medical research. This is relevant to fields such as drug discovery, personalised medicine, and surgery planning.\n",
        "Benefits:\n",
        "Learn how to use generative and agentic AI to identify diseases, develop new drugs, and personalise treatment plans.\n",
        "Gain a deeper understanding of the ethical implications of using generative AI in healthcare.\n",
        "Prepare for a career in a growing field with high demand for skilled professionals.\n",
        "\n",
        "AI-702: Web3, Blockchain, and Agentic AI Integration\n",
        "Integrating Web3, blockchain, and agentic AI technologies creates a powerful ecosystem where autonomy, security, transparency, and user control are at the forefront. These technologies combined enable:\n",
        "Secure, decentralised AI applications that protect user privacy,\n",
        "Transparent AI interactions that foster trust,\n",
        "Autonomous, scalable AI agents that operate reliably in peer-to-peer environments,\n",
        "Tokenized incentives that drive community participation, and\n",
        "Innovative ownership models for digital assets, enabling new economic opportunities.\n",
        "\n",
        "\n",
        "AI-703: Metaverse, 3D, and Agentic AI Integration: This specialisation will teach students how to create and use 3D models and other immersive content manually and with generative AI. This is relevant to fields such as gaming, marketing, and architecture.\n",
        "Benefits:\n",
        "Learn how to use generative AI to create realistic and immersive 3D models.\n",
        "Develop the skills necessary to work in the growing field of virtual reality (VR) and augmented reality (AR).\n",
        "Apply generative AI to solve real-world problems in areas such as product design, marketing, and education.\n",
        "\n",
        "\n",
        "AI-704: Agentic AI for Accounting, Finance, and Banking: This specialisation will teach students how to integrate generative AI with Web3 and blockchain technologies. This is relevant to fields such as finance, healthcare, and supply chain management.\n",
        "Benefits:\n",
        "Learn how to create smart contracts and decentralised applications (dApps).\n",
        "Gain a deeper understanding of the potential of blockchain technology and how it can be used to improve business processes.\n",
        "Develop the skills necessary to work in a rapidly growing field with high demand for skilled professionals.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "AI-705: Agentic AI for Engineers: This specialisation will teach students how to use generative AI to improve engineering design and problem-solving. This is relevant to fields such as manufacturing, construction, and product development.\n",
        "Benefits:\n",
        "Learn how to use generative AI to create simulations, optimize designs, and predict failures.\n",
        "Gain a deeper understanding of the engineering design process and how generative AI can be used to improve it.\n",
        "Prepare for a career in a growing field with high demand for skilled professionals.\n",
        "\n",
        "AI-706: Agentic AI for Sales and Marketing: This specialisation will teach students how to use generative AI to improve sales and marketing campaigns. This is relevant to fields such as advertising, public relations, and customer service.\n",
        "Benefits:\n",
        "Learn how to use generative AI to create personalised marketing messages, generate leads, and track campaign performance.\n",
        "Gain a deeper understanding of the latest marketing trends and how generative AI can be used to improve them.\n",
        "Prepare for a career in a growing field with high demand for skilled professionals.\n",
        "\n",
        "AI-707: Agentic AI for Automation and Internet of Things (IoT) :\n",
        "Provide Multi-Modal User Interface for the IoT systems: Multimodal interaction exploits the synergic use of different modalities to optimise the interactive tasks accomplished by the users. This allows a user to use several input modes such as speech, touch, and visual to interact with IoT systems.\n",
        "Improve efficiency and accuracy of industrial processes: By implementing GenAI in automation and IoT systems, industries can optimise their processes, reduce manual labour, and increase productivity while ensuring higher accuracy and consistency.\n",
        "Enhance decision-making: GenAI can analyse vast amounts of data collected by IoT sensors to derive valuable insights, enabling businesses to make informed decisions regarding operations, maintenance, and resource allocation.\n",
        "Personalise user experiences: GenAI can leverage IoT data to understand user preferences and behaviours, enabling the creation of personalised experiences across smart devices and IoT-enabled systems.\n",
        "\n",
        "AI-708: Agentic AI for Cyber Security:\n",
        "Strengthen threat detection and response: GenAI can be used to rapidly detect and respond to cyber threats by analysing large volumes of security data in real time, identifying anomalies, and suggesting appropriate countermeasures.\n",
        "Enhance security monitoring and analysis: GenAI can assist security analysts in monitoring and analysing security logs, automating threat detection, and providing insights into security risks and vulnerabilities.\n",
        "Improve threat intelligence: GenAI can be used to gather and analyse threat intelligence from various sources, enabling organisations to stay informed about the latest threats and trends and proactively strengthen their security posture.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Initialize LLM with tools\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "\n",
        "# System message for the agent\n",
        "SYSTEM_PROMPT = f\"\"\"You are a helpful Panaversity assistant with access to the following knowledge:\n",
        "\n",
        "{PANAVERSITY_KNOWLEDGE}\n",
        "\n",
        "Help users by:\n",
        "1. Understanding their queries about Panaversity\n",
        "2. Providing accurate information from the knowledge base\n",
        "3. Being professional and friendly\n",
        "4. Maintaining context across the conversation\n",
        "5. Asking for clarification when needed\n",
        "\n",
        "If information is not in the knowledge base, politely say so and suggest contacting Panaversity directly.\"\"\"\n",
        "\n",
        "sys_msg = SystemMessage(content=SYSTEM_PROMPT)\n",
        "\n",
        "def create_panaversity_agent():\n",
        "    \"\"\"Create a Panaversity agent with memory.\"\"\"\n",
        "\n",
        "    # Define the assistant node\n",
        "    def assistant(state: MessagesState) -> MessagesState:\n",
        "        \"\"\"Process messages and generate responses.\"\"\"\n",
        "        # Get all messages including system prompt\n",
        "        messages = [sys_msg] + state[\"messages\"]\n",
        "\n",
        "        # Generate response\n",
        "        response = llm.invoke(messages)\n",
        "\n",
        "        return {\"messages\": [response]}\n",
        "\n",
        "    # Create the graph\n",
        "    builder = StateGraph(MessagesState)\n",
        "\n",
        "    # Add nodes\n",
        "    builder.add_node(\"assistant\", assistant)\n",
        "\n",
        "    # Add edges\n",
        "    builder.add_edge(START, \"assistant\")\n",
        "    builder.add_edge(\"assistant\", END)\n",
        "\n",
        "    # Create memory saver\n",
        "    memory = MemorySaver()\n",
        "\n",
        "    # Compile graph with memory\n",
        "    return builder.compile(checkpointer=memory)\n",
        "\n",
        "class PanaversityAgent:\n",
        "    def __init__(self):\n",
        "        self.graph = create_panaversity_agent()\n",
        "        self.thread_id = str(datetime.now().timestamp())\n",
        "\n",
        "    async def process_message(self, message: str, thread_id: Optional[str] = None) -> str:\n",
        "        \"\"\"Process a message and return response.\"\"\"\n",
        "        if thread_id:\n",
        "            self.thread_id = thread_id\n",
        "\n",
        "        # Create config with thread_id\n",
        "        config = {\"configurable\": {\"thread_id\": self.thread_id}}\n",
        "\n",
        "        # Create message\n",
        "        messages = [HumanMessage(content=message)]\n",
        "\n",
        "        # Invoke graph with memory\n",
        "        result = self.graph.invoke(\n",
        "            {\"messages\": messages},\n",
        "            config\n",
        "        )\n",
        "\n",
        "        # Extract and return response\n",
        "        return result[\"messages\"][-1].content\n",
        "\n",
        "    def new_conversation(self) -> str:\n",
        "        \"\"\"Start a new conversation thread.\"\"\"\n",
        "        self.thread_id = str(datetime.now().timestamp())\n",
        "        return self.thread_id\n",
        "\n",
        "    def update_knowledge_base(self, new_knowledge: str):\n",
        "        \"\"\"Update the knowledge base.\"\"\"\n",
        "        global PANAVERSITY_KNOWLEDGE\n",
        "        PANAVERSITY_KNOWLEDGE += f\"\\n\\n{new_knowledge}\"\n",
        "\n",
        "        # Update system prompt\n",
        "        global SYSTEM_PROMPT\n",
        "        SYSTEM_PROMPT = f\"\"\"You are a helpful Panaversity assistant with access to the following knowledge:\n",
        "\n",
        "        {PANAVERSITY_KNOWLEDGE}\n",
        "\n",
        "        Help users by:\n",
        "        1. Understanding their queries about Panaversity\n",
        "        2. Providing accurate information from the knowledge base\n",
        "        3. Being professional and friendly\n",
        "        4. Maintaining context across the conversation\n",
        "        5. Asking for clarification when needed\n",
        "\n",
        "        If information is not in the knowledge base, politely say so and suggest contacting Panaversity directly.\"\"\"\n",
        "\n",
        "        global sys_msg\n",
        "        sys_msg = SystemMessage(content=SYSTEM_PROMPT)\n",
        "\n",
        "# FastAPI implementation\n",
        "# from fastapi import FastAPI, WebSocket\n",
        "# from fastapi.responses import HTMLResponse\n",
        "\n",
        "# app = FastAPI()\n",
        "\n",
        "# html = \"\"\"\n",
        "# <!DOCTYPE html>\n",
        "# <html>\n",
        "#     <head>\n",
        "#         <title>Panaversity Chat</title>\n",
        "#         <style>\n",
        "#             #chat-container {\n",
        "#                 max-width: 800px;\n",
        "#                 margin: 20px auto;\n",
        "#                 padding: 20px;\n",
        "#                 border: 1px solid #ccc;\n",
        "#                 border-radius: 8px;\n",
        "#                 box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
        "#             }\n",
        "#             #messages {\n",
        "#                 height: 500px;\n",
        "#                 overflow-y: auto;\n",
        "#                 margin-bottom: 20px;\n",
        "#                 padding: 15px;\n",
        "#                 border: 1px solid #eee;\n",
        "#                 border-radius: 5px;\n",
        "#             }\n",
        "#             .input-container {\n",
        "#                 display: flex;\n",
        "#                 gap: 10px;\n",
        "#             }\n",
        "#             #messageInput {\n",
        "#                 flex-grow: 1;\n",
        "#                 padding: 8px;\n",
        "#                 border: 1px solid #ddd;\n",
        "#                 border-radius: 4px;\n",
        "#             }\n",
        "#             button {\n",
        "#                 padding: 8px 15px;\n",
        "#                 background-color: #0066cc;\n",
        "#                 color: white;\n",
        "#                 border: none;\n",
        "#                 border-radius: 4px;\n",
        "#                 cursor: pointer;\n",
        "#             }\n",
        "#             .message {\n",
        "#                 margin: 10px 0;\n",
        "#                 padding: 10px;\n",
        "#                 border-radius: 5px;\n",
        "#                 max-width: 80%;\n",
        "#             }\n",
        "#             .user {\n",
        "#                 background-color: #e3f2fd;\n",
        "#                 margin-left: 20%;\n",
        "#             }\n",
        "#             .assistant {\n",
        "#                 background-color: #f5f5f5;\n",
        "#                 margin-right: 20%;\n",
        "#             }\n",
        "#             #newChat {\n",
        "#                 margin-bottom: 10px;\n",
        "#                 background-color: #28a745;\n",
        "#             }\n",
        "#         </style>\n",
        "#     </head>\n",
        "#     <body>\n",
        "#         <div id=\"chat-container\">\n",
        "#             <h2>Panaversity AI Assistant</h2>\n",
        "#             <button id=\"newChat\" onclick=\"startNewChat()\">Start New Chat</button>\n",
        "#             <div id=\"messages\"></div>\n",
        "#             <div class=\"input-container\">\n",
        "#                 <input type=\"text\" id=\"messageInput\" placeholder=\"Ask me anything about Panaversity...\">\n",
        "#                 <button onclick=\"sendMessage()\">Send</button>\n",
        "#             </div>\n",
        "#         </div>\n",
        "#         <script>\n",
        "#             let ws;\n",
        "#             let threadId;\n",
        "\n",
        "#             function connect() {\n",
        "#                 ws = new WebSocket(`ws://localhost:8000/ws/${threadId}`);\n",
        "\n",
        "#                 ws.onmessage = function(event) {\n",
        "#                     addMessage('assistant', event.data);\n",
        "#                 };\n",
        "#             }\n",
        "\n",
        "#             function startNewChat() {\n",
        "#                 if (ws) {\n",
        "#                     ws.close();\n",
        "#                 }\n",
        "#                 threadId = Date.now().toString();\n",
        "#                 connect();\n",
        "#                 document.getElementById('messages').innerHTML = '';\n",
        "#                 addMessage('assistant', 'Hello! How can I help you learn about Panaversity?');\n",
        "#             }\n",
        "\n",
        "#             function addMessage(role, content) {\n",
        "#                 let messages = document.getElementById('messages');\n",
        "#                 let message = document.createElement('div');\n",
        "#                 message.className = `message ${role}`;\n",
        "#                 message.textContent = role === 'user' ? 'You: ' + content : 'Assistant: ' + content;\n",
        "#                 messages.appendChild(message);\n",
        "#                 messages.scrollTop = messages.scrollHeight;\n",
        "#             }\n",
        "\n",
        "#             function sendMessage() {\n",
        "#                 let input = document.getElementById('messageInput');\n",
        "#                 let message = input.value.trim();\n",
        "\n",
        "#                 if (message) {\n",
        "#                     addMessage('user', message);\n",
        "#                     ws.send(message);\n",
        "#                     input.value = '';\n",
        "#                 }\n",
        "#             }\n",
        "\n",
        "#             document.getElementById('messageInput').addEventListener('keypress', function(e) {\n",
        "#                 if (e.key === 'Enter') {\n",
        "#                     sendMessage();\n",
        "#                 }\n",
        "#             });\n",
        "\n",
        "#             // Start first chat\n",
        "#             startNewChat();\n",
        "#         </script>\n",
        "#     </body>\n",
        "# </html>\n",
        "# \"\"\"\n",
        "\n",
        "# @app.get(\"/\")\n",
        "# async def get():\n",
        "#     return HTMLResponse(html)\n",
        "\n",
        "# @app.websocket(\"/ws/{thread_id}\")\n",
        "# async def websocket_endpoint(websocket: WebSocket, thread_id: str):\n",
        "#     await websocket.accept()\n",
        "#     agent = PanaversityAgent()\n",
        "\n",
        "#     try:\n",
        "#         while True:\n",
        "#             message = await websocket.receive_text()\n",
        "#             response = await agent.process_message(message, thread_id)\n",
        "#             await websocket.send_text(response)\n",
        "#     except Exception as e:\n",
        "#         print(f\"WebSocket error: {e}\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     import uvicorn\n",
        "#     uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "\n",
        "def assistant(state: MessagesState) -> MessagesState:\n",
        "   return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n"
      ],
      "metadata": {
        "id": "xjXRIDlmDSly"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import START, StateGraph\n",
        "from langgraph.prebuilt import tools_condition, ToolNode\n",
        "from IPython.display import Image, display\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "\n",
        "# Graph\n",
        "builder: StateGraph = StateGraph(MessagesState)\n",
        "\n",
        "# Define nodes: these do the work\n",
        "builder.add_node(\"assistant\", assistant)\n",
        "builder.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "# Define edges: these determine how the control flow moves\n",
        "builder.add_edge(START, \"assistant\")\n",
        "builder.add_conditional_edges(\n",
        "    \"assistant\",\n",
        "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
        "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
        "    tools_condition,\n",
        ")\n",
        "builder.add_edge(\"tools\", \"assistant\")\n",
        "react_graph: CompiledStateGraph = builder.compile()\n",
        "\n",
        "# Show\n",
        "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "3z7Zyy8-D9rl",
        "outputId": "77ae61ef-a7c9-4c89-eb10-92deab212c50"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAANYAAAD5CAIAAADUe1yaAAAAAXNSR0IArs4c6QAAIABJREFUeJztnXdcU1fj/89NQnYChD1kiQgIjooTXFXqI44fUKt11Grr86271tX66GPt0Nplfdo+1rb6WBXrnlgVrKsuXBUVEESmjEBISEJCxk1yf3/EF6UYhpp7zw0571f/sMnNOZ/Am3PvPfcMjCAIgEDAgwE7AMLZQQoiIIMUREAGKYiADFIQARmkIAIyLNgBnge1AlfL8Ua1WdtgMhkdo1uJ5YIxWRhfxOSLWR5+bC6fCTsRXcAc4xcIAABAVqkvuqstydUKxCyzieCLmQIRi81jAEf4BiwOpqk3NTaYG9UmrcoscGWGxgi69RYK3V1gR4OMYyiokuNXj9cxXTB3b3ZoD4FnAAd2ohelskhXkqNVSA1uXuzB4z1YLs57ReQACl4/JS+41TB4gmd4LyHsLPbn7h/Kq+nyISmeMYNdYWeBA90VPPifiph4cWScGHYQcrmRoWhQ4COn+MAOAgH6KkgQxE8riye84+8XyoOdhQryrqtLc7VJb/nBDkI19FXwhxWPZqwOEYgd8p79+ci/qc65qp74biDsIJRCUwUPbqqIT/bwC3GK9q8596+o5FWG4a95ww5CHXS8Ecs6KY8dInZC/wAAsfGufBHzwQ017CDUQTsF62uNj7I13ft28vuPNnhppPuFAzLYKaiDdgpeTZcPHu8BOwVMWC6MvqPcr5+Sww5CEfRSUFqq5/AYYbGdsP/vmeg/WiIt1eNGC+wgVEAvBYvuaSS+bMqqy8nJMRgMsD7eNlwBsyRHS1LhtIJeCpbkakN7CKipKz09febMmTqdDsrH2yU0RoAUpJr6WqNYwnL3oagVfO4GzNqNRV77ZyUsVqCS46RWQRNopKCqDscwjIySy8rK5syZk5CQkJSUtH79eovFkp6evmHDBgDAqFGj4uLi0tPTAQDZ2dkLFixISEhISEh45513Hjx4YP24UqmMi4vbtWvX6tWrExIS/vnPf9r8uH1huTA0SpNWZbJ7yXSDRs8eGtVmvpiUUXSffPJJaWnp0qVLtVrtrVu3GAxGfHz89OnT09LSNm3aJBQKg4KCAABVVVUGg2H27NkMBuPAgQOLFi1KT0/ncrnWQrZt2/baa69t2bKFyWT6+Pg8/XG7IxCztGqTwJVGvyMyoNHX06pNJD2Oq6qqioyMTElJAQBMnz4dACCRSAIDAwEAMTExbm5u1sPGjBmTlJRk/Xd0dPScOXOys7MHDhxofSU2Nnb+/PlNZT79cbsjcGVqVWbQhaTi6QKNFASAYHFIOREnJSX98ssvX3zxxezZsyUSSWuHYRh2/vz5tLS0kpISPp8PAJDL/+qc69+/PxnZ2oDDZRIWOj4+tS80uhbkCVgNClIufebPn79kyZLMzMwJEybs37+/tcO2bt26fPny6OjojRs3Ll68GABgsfzVM8fjUf3AUFln5DvBKA0aKcgXMxvVZjJKxjBs6tSpx44dGzZs2BdffJGdnd30VtMoDYPBsH379uTk5KVLl/bu3Ts2NrYjJZM6yIO8i2NaQSMFRRIXF3JOxNYOFIFAMGfOHABAfn5+U6smkz15GqvT6QwGQ1RUlPV/lUpli1awBS0+TgYiCUvk1vlbQRp9Q68ATuUjnUZpEtr75/7+++8LhcKBAwdevnwZAGD1rFevXkwm86uvvpowYYLBYHj11VfDw8P37t3r4eGh0Wh++uknBoPx6NGj1sp8+uP2zVyap3VhMzAGKX+TtIK5du1a2Bn+QinDcb3FO4hr32IrKiouX758+vRpnU63cOHC4cOHAwDEYrGPj8+ZM2cuXbqkVqvHjRv30ksvXblyZf/+/WVlZQsXLgwODj506NC0adNwHN+5c2dCQkJ0dHRTmU9/3L6Z75xXBoTzvLvY+UdBQ+g1ZLU8X1ucox0+0YkGbLZG+k9VIyZ5Cd06/xRPGp2IAQBBkYLrpxTSMr1vsO2/fqVSmZycbPOtwMDAioqKp18fNmzYRx99ZO+kLZk9e7bNs3ZUVFTTU5bm9O3b9+uvv26ttJyrKqEbyxn8o10rCACofKS7flqeusD2/Amz2VxTU2PzLQyz/V14PJ67u7u9Y7ZEJpPhuI1Huq2l4nA4Hh6tDov8aWXxm2uCObzOfztMRwUBAOf313brIwzsxocdBA73r6iMekvfkaT/2dAEGnXKNDFikvfpHVKdhpQ+QppTXtBYfE/jPP7RVEEAwJQVQb9+Xg47BdU01ONn0mr+39wA2EEohY4nYisGnXn3hvJpHwQ5ySVRTZk+M61m2soghhP0BTaHvgpaW4U9Xzye8I6fb2ef0FlwW333D9Wk9zr7qBhb0FpBK2f31Oi05vjxnpQNqKaSisLGK+nywHBe/ARP2Fng4AAKAgBKcrRX0uvCYgU+QdzQGEEnOFXpteaSXG11iV5Vh8eP97D7AyEHwjEUtFJ4p6HwjqYkRxs1QMxiYwIxS+DK5HCZDvEFmExMqzY1qk0alUmtMNWU6UN7CCL6ioK6O2nfUxOOpGATpQ+0qlpcqzZpVWaTyWKxa+8NjuN5eXm9evWyZ6EA8IRMwkLwxSyhK8vDj+3ftZNf3XYch1SQVORy+ZQpUzIzM2EHcRZo2i+IcB6QggjIIAVbgmFYREQE7BROBFKwJQRBPHz4EHYKJwIp2BIMw1xdnXTxeyggBVtCEIRKpYKdwolACtrA19cXdgQnAiloA6lUCjuCE4EUbAmGYc1nyiHIBinYEoIg8vLyYKdwIpCCCMggBVuCYVgbq28h7A5SsCUEQSgUCtgpnAikoA08PZ10ADMUkII2qKurgx3BiUAKIiCDFGwJhmFdu3aFncKJQAq2hCCIoqIi2CmcCKQgAjJIQRs0LfeLoACkoA1srgiIIAmkIAIySMGWoJEyFIMUbAkaKUMxSEEEZJCCLUGTOCkGKdgSNImTYpCCCMggBVuC5hFTDFKwJWgeMcUgBVuCRspQDFKwJWikDMUgBRGQQQrawMfHB3YEJwIpaIPWdlpEkAFS0AZovCCVIAVtgMYLUglSsCVosBbFIAVbggZrUQxS0AaBgbb3hEeQAdr65glvv/22VCplMpkWi6W+vl4ikWAYZjKZTp48CTtaJwe1gk+YNGlSQ0NDVVWVVCo1GAzV1dVVVVUY5vD7LdIfpOATRo8eHRYW1vwVgiD69u0LL5GzgBT8iylTpvD5f+2L6evrO3XqVKiJnAKk4F+MHj06ODjY+m9rExgZGQk7VOcHKfg3ZsyYIRAIrE3glClTYMdxCpCCfyMxMTE4OJggiD59+qDHdNTAgh3ABhYLoZTh6jrcAqO/KPmVd0Dj0X8MfbM4R0t97UwmcPdmiz1cqK8aFrTrF8y/pc69qm7UmP3D+FqVCXYcqhG6s8rzte5e7H6j3f3DnGLndnop+OC6uvCudthrvgyGU3fI6XXmzB2ViVO9vbtwYWchHRpdCxZmawr+1IyY7Ofk/gEAuDzmhDlBp36RKmVG2FlIh0YK3rukjE9Gw5X/YtB471uZ9bBTkA5dFNRpzYpqI5fPhB2ERrh6sssLGmGnIB26KNigwH2CnOLqu+PwRSwun2kyWmAHIRe6KAgApm1wuvvfdlHJ8U4/VII+CiKcFKQgAjJIQQRkkIIIyCAFEZBBCiIggxREQAYpiIAMUhABGaQgAjJIQQRknFrBk6eOJaeOqqmRtnaA2Wy+fz/7xSuSSqurpVUvXk6nxKkVZLM5AoGQwWj1h/Dl159s3LT+BWuprKqYOn1CQQFaKsk2dJy+RBmjRv5j1Mh/tHGA0WB48VrMJhOtZkfQDQdW8P797F1pW+/nZAMAIrv3mDNncfeIKACAXq/f9O2Gq1f/AAD07Nlnwbxlvr5+WVmXf9r6XVVVha+v/4TxE1NTJm/4Ym1GxgkAwJmMLBaLZfOA8xfOAABGjIwDAPy6+7ifr/+p08ePHt1fXPKIx+P37zdowfxlbm7uAICDh349dz7ztYnTtm37r1xR161b5LIlq4OCQqqlVW/OmggA+OjjDz4CYPTocR+sWAv7J0cvHFhBqbTKYDS8MX02g8E4duzABysX7dmdzuVyf92zPSPjxKyZczw8PDMyT/B4vMbGxrUfvx8SHLZ0yeqSkkdyuQwAkJryusViOXPmJADA5gHTp74lq62prq5c+cHHAAAPiScAIC/vflBQSGJiUn294vCRvdpG7WfrNlnzPHiQs3//rqVLV5tMpo0b1332+Yc//HeHh8Rz1b8+Xbd+9ayZc/r0jnN3l8D+sdEOB1Zw1KgxiYlJ1n937x69ZOmc+znZ/eIGVkureDze1CkzWSzW2KRk69WYwWAYMuTlxFFjmj4e0S0yJPjJOkb1SsXTBwQGBrm6uinq5bGxvZteXPLev5rGkLJYrLTd/zMYDBwOx/rKuk+/kUg8AACpqa9v/uEblVrlKnaN6BYJAAgKCmleDqIJB1YQw7BLl8/vP5BWVlZiXY6oXiEHAIwaOebs2dPvf7Bw/rylYWHhAAB/v4AePXqm7d7G5fLGj0tls9ktimr3gCZwHD98ZO+Z30/W1ko5HK7FYlEq6318fK3vcrlP5h74+PgBAOR1Mlcx2s6uHRz4jnjnrq1rPlzePSJ63Scb57yzGABgISwAgAH9B3+2/j+Kevnb/3z9q68/NZlMGIZtWP/t6FfGbflx04yZqXfv/tmiqHYPsEIQxL9WLd796//G/GPC5xu+TxyV1FRpC1xYLgAAs8VMzlfvVDiqgjiO/7pn+9ik5AXzl8bG9o6Oim3+7oD+g7f9vHfe3Pd+O3l0z94dAAChULj43Q92/HJIIBCu/veSxsaWM9NaO6D5zezdu3/e/vPGu4s+mPjq1OiomLDQcEq+ayfHURU0Go0GgyEi4snKQyq1EgBgsVisbwEAGAzGaxOneXp6FRbmAwAMBoP1hJua8rpGq5E+1VFs8wAul6dQyK3FNtVivbZrUWkbcDhc60mZhB9DZ8BRrwUFAkFYWPjhI3slEg+tRrNj508MBqO4+BEA4PCRvVeuXkwclSSXy+rqZN27R+M4/uasV4cPSwwN6Xrs2AGhQOjv/7cFzVs7oFfPl06dPr7xm/WxMb1FInF0VCybzf556/djx6YUFxf+umc7AKCk+FGAf1vLo3t7+/j7Bew/mMbl8dRq1eRJb7TRGe6EOPDP4t+r1vO4vI8/WbnvwK65c997Y/rbGRnpOI77+wfiRuMPW7757eTR1NTXJ096Q6fX9end7/ezpzZ9u4Hl4rJ+3SYu929rtbR2QGJiUkrypAsXz/y09bvcvHteXt6rV60rfJS/9qMVt29f3/j1jwMHJhw+srftnBiGrV69ns8XfP/fr05npFsbaUQTdFnWqPax4eze2nH/1wV2EHqR9mnR/60PY7p05qnEDtwKIjoHSEEEZJCCCMggBRGQQQoiIIMUREAGKYiADFIQARmkIAIySEEEZJCCCMggBRGQQQoiIEMXBRlMTCxx1MGL5OEVyGEwO/MwGRop6OnPLsnV0mTkGE1QSA24wYLR5VdEFjT6fpH9RNUlOtgpaERNua5bHyHsFKRDIwVHTPK+fLhGp0Ub4AAAQGluQ2lOQ1xi55/6TpdR01YMOvOudeW9R0iEbi5u3mxAo2gUQQCgqNY3yPHyfM1r7wV2+q2XaKeglVu/KyoKdQSBqVrZCtVsNuM43mL+h70gCEKv1/N4FG2Ip9PpOBxO04QmzwAOACA4kheb4EZNAPgQDsjChQvJK3zTpk0JCQnHjx8nr4rm1NbWrlmzhpq66AkdW8E2OHfu3Msvv0xe+dXV1QsXLiwtLY2Kitq1axd5FT3Nzp07R44cGRAQQGWldIBGtyPtMnnyZLJ/QwcOHCgtLQUAlJeXnzhxgtS6WpCUlDR37lyDPVY0dCwcoxWUSqWurq6VlZXh4SSuoVFZWblo0aKysjLr/1LfEFovDe/duxcdHS0SiSiuGhYO0AoeOHAgKyuLx+OR6h8A4MiRI03+AQDKysqOHTtGao1Pw+PxunXrNn78eI1GQ3HVsHAABcvKypKTk8mupaqq6vz5881f0Wq1u3fvJrvep5FIJBcuXNDr9VJpq+uwdyZoreDVq1cBAMuWLaOgrr1791qbwKZlijAMe/z4MQVV28TT01MoFMbHxzdvmDsnsG/JbWM0GgcPHlxfX0991TKZ7JVXXqG+XpvodLrt27fDTkEudGwFlUplWVnZ2bNn3dwgdM+azebIyEjq67UJl8udOXMmAGDVqlVmc+dcMJN2Ch4/fry0tDQ8PJykhx/tguO4tV+GVsyaNWvx4sWwU5ACvRSUyWR37tzp3RvmsuA6nc7HxwdiAJuEh4d/9913AIALFy7AzmJnaKRgaWkphmEffvgh3BhyudzFxQVuhjbAcXzFihWwU9gTuii4Zs0aHo/n6ekJOwior68PCgqCnaJVEhMTx44dCwAwmTrJqDZaKFhRUTFgwACanP5KSkro8JfQBsOGDQMA7Nu37+HDh7Cz2AH4Cup0OqFQaP3LpgMGg6Fr166wU7TPtGnTPvzww05wmwxZweXLl1+7dg1K50trnDt3LiIiAnaKDrFnzx6TyVRQUAA7yAsBU8Hbt28vWrSI1MFXz4pSqRSLxf7+/rCDdBQOh6NQKHbu3Ak7yPMDTUGFQtGtW7cuXei1vnlWVlZISAjsFM/GoEGD6uvrYad4fuAoePDgwR9//FEsFkOpvQ3++OOPoUOHwk7xzLz77rvWvYBgB3keICgolUrd3NxWrlxJfdXtolKpHFFBAACbzd68eXNaWhrsIM+MYwxZpYaMjIyLFy+uX78edpDn5/r1656eng5xR98E1a3gggULcnJyKK60gxw5ciQlJQV2ihdiwIABwcHB7W6LRysoVfDixYvjx4+PiYmhstIOUlJSwmKx+vXrBzvIi8JisRITE5VKJewgHQWdiJ+wbNmysWPHjhgxAnYQO6BSqU6cODFt2jTYQToEda3gvn37aHsKzs/Pr66u7hz+AQBcXV0dxT/qFCwtLd2/fz89T8EAgG+++Yaa6QFUsnz58rt378JO0T4UKYhh2NatW6mp61k5evRoYGBgnz59YAexM8uXL//2229hp2gfZ78WNJlMo0ePPnv2LOwgzgsVreC5c+c+/vhjCip6DpYsWULbbHYhMzMTdoR2oELBrKysQYMGUVDRs7Jr166wsLD4+HjYQUjk4cOH27dvh52iLZz3RFxYWPjdd985xNXSi2AymdLT0+nc5U6Fgkajkc1mk13Ls9K/f/9r164xmUzYQZwd0k/Eubm5s2fPJruWZ2X69Ok7duxwEv9ycnI2b94MO0WrkK6gRqMhezmiZ+X777+fNm1aVFQU7CAUERMTs3v3br1eDzuIbZzuWnDr1q04js+dOxd2EEqpqKgQCATu7u6wg9iA9FbQZDIZjbaXjKae48ePV1ZWOpt/AIDAwEB6+keFgufOnYM+O93KzZs3c3NzaRKGYmpra+fNmwc7hW1I33PLw8ODDsPX7t27t3nzZpr3kJGHt7d3QUGBUqmk1WRFK05xLVhUVLRy5cr9+/fDDgITi8WCYRgNNzLp/P2CFRUVixYtOnz4MKwAiLah4gFdSkoKrDVrCwsL582bh/yz3or98MMPsFPYgIr9V4cPH/7mm2+azWa1Wu3t7U3ZZgr5+fl79+49fvw4NdXRHJFIVFRUBDuFDUhUcOjQoY2Njda1hK2XIARBREdHk1djc4qKilatWnXo0CFqqqM/Q4YM6dWrF+wUNiDxRPzyyy9bt1ZrugTmcDgDBgwgr8YmcnJyfv75Z+Rfc1gslkRCx309SVRw7dq10dHRzW93vLy8KPhDzM7O/vLLLzds2EB2RY6FTCYbN24c7BQ2IPd25PPPP29aooUgCD6fT/bz4kuXLp04cWLHjh2k1uKIsNls63UR3SBXQR8fn/fee8+6YiSGYWQ3gRkZGYcOHVq9ejWptTgoYrGYntN3SO+USUhISE1NFQgEQqGQ1AvBo0ePXrx4cdOmTeRV4dBgGBYWFgY7hQ06dEdswi06zfM/ZJvy2ltlRbVFRUVhQT0a6klZIfn8+fO594sdejkYssFxfOLEidTvqtcu7TwdeXBDfe+SSiE18oQvNLqzqV+GJIxGo3eAsKqoMaynsF+iu4c/h7y6HIvly5efPXu2qVPM2hwSBPHnn3/CjvaEtlrBG5mKuip8SKqvSELfTRCaYzETSpnx5C/SUVN9/ELg7JxDN+bOnZuXl1dTU9O8d4xWy3i2ei14/bRCJTMNSfFxFP8AAAwmJvHlJM8PPruntqacpoOEKSYsLKxv377Nz3UYhtFqDUXbCtbXGusqDQPHeVOexz68PMXvVqYDr31rX2bMmNF8Q43AwMDXX38daqK/YVvBukoDQdBuVE/HEbm7PC5sNBrgj1OkA+Hh4f3797f+myCIIUOG0GSLFyu2FdSozF5dHPtaKjhaoKh2yLWXyeCNN97w9vYGAAQEBNBt0S3bCuIGC6537CZELTcB4MANuX3p2rXrgAEDCIIYNmwYrZpAigZrIZ4Vi4Uoz2/U1Ju0apMJJ3RaO2yx1Mt/ur5Pt+6S+N/31Lx4aVwek81j8MVMsbtLUCT/RYpCCtKLBzfUBbc1FYWN/hFik5FgujAZLiyA2aNTgsHtP2gsbgG4PR4UN2gIM24ym3AXF8PxH6uCowURfYTd40TPURRSkC7kXVdfPlbnFSRiCUQxifQ6V7aNe7CkobYx97b+Srp8SLJHtz7PJiJSED46jfnk9hrczAgbEMhiO94aIxiGiX0EAAiEXuJb5xQPbmrGvu3LZHb0Qhz+TpxOTnmBdue6MmGAxLe7lyP61xw2j+UX7c12d9uyoqj2cUcfDSAFYVLzWH/xsKL70GAOz2EeQbULV8juMSr05PYatbxDq2ggBaFRkqvJTJN16e0wu34+EyH9Ag9vlkrL2m8LkYJw0ChNZ/d0Wv+shMQFHP6u0oS308GMFITD6Z01If0DYKcgna4D/X/7XzvdkEhBCNw6U28GbJaLY998dASOgK3VYrnXVG0cgxSEQNZJuXc4TZdaszveYZIr6Yo2DrCngnkPcl5wV+YLF38fMTKuvLzUfqFox+3fFQHREhouLwQA+PiLcQeP2XnyK4vD9AgS5VxttSG0m4KnM9LnL5ip1+vsVWBn5cFNDdfVsUchPSscITf/lqa1d+2moIPuSk8xagWu11p4Iuea2iL04Mke6/FWhm/a5wHd6Yz0Tf/ZAABITh0FAHh/xYf/GD0eAJCZ+dvuPdurqio8PDzHJqVMmzrLusSHyWTa/suWjMwTKpUyODh05pvvJMQPf7rYrKzLP239rqqqwtfXf8L4iakpk+2SFiKPCxrdA4UkFf6o+PbJM5urpA9FQkl4aNyYxLlikScAYPW6ka+Ofz/nwYW8gis8rnBgv5RXRjzZA8FsNv9+YVvWraNGo65rWF8cJ2u2g2eIqOxBY3hvG9/dPq3ggP7xk16bDgD4bN2mbzdtHdA/HgCQkXHis88/7NYt8t+r1w8flvi/7T/s/vXJIqdfff3pvv27xo1NWfWvT319/f+9Ztm9e3dalNnY2Lj24/fZLuylS1YPHjRULpfZJSpc6qpxgiDlFrCw6ObPOxf5eIdOSl41dPDU4tI7W7bPNxqfKLX38Ef+vhHz3t7yUq8xmed+ziu4Yn39yIkvz1zYFhkxOGXcMrYLV6dvICMbAMBsxuplth+W2KcVdHeX+PsHAgCiomJcXd2sA8S3/u+/sbG9V//rUwDA0CEvNzSo9+7b8WrqlLq62ozMEzPemD3zzXcAAMOGjpw+I+WXHT9u/HpL8zLrlQqDwTBkyMuJo8bYJSQd0KpMLA6PjJKP/vb1wLiUlHFPtrSNCB/w5beTCx5lxUYPBwD0f2nCyGEzAQD+vhE3bh97+Cgrunt8RVV+1q0jI4fNGjNqDgAgrs/YohKyZna6cFiaVqaQkzVSpqKivK5ONnnSG02v9Os36OSpYxWV5QUFeQCAhIQn+09jGNYvbuCZ30+2KMHfL6BHj55pu7dxubzx41JpuH/Tc6DTmDnu9u8OVNRX18hK6hSPs24dbf66UvWkW5jNfuI9k8l0FXur1DIAwP28CwCAoYOnNB2PYWR10rE4jEY1tQpqtBoAgJvbX6uJiURiAECdrFar1QAA3Ju9JRa7NjY2arXa5iVgGLZh/bdbt32/5cdNBw6mrXz/4169XiIpLWWQtKpyg0YOAEgcMbtn9N82lheJPJ8+mMFgWSxmAIBSKeVyhQK+KymZWkBglla+u52tb5qv6u3lAwBQqZRNb9XXK6wienp6AwDU6r86ihQKOYvF4nJbdlUIhcLF736w45dDAoFw9b+X0HNhqGdC4Mo0GewwCr8FPK4IAIDjBm+vkOb/8bht3foIBO56vQY3UbErjMlgErnbbu/spiCPywMA1NU9uWnw8PD09fG7ceNK0wEXL/7O5XLDw7tHRcVgGJZ1/bL1daPRmHX9co8ePZlMJtuF3dxOa0ePv19AasrrGq1GKq2yV1pYiFxZJqP9FfTyDHJz9b35Z7rB+KRf1mw2mUx4258KDIgEANy5l2H3PE9jMppFbrYVZK5du/bpVyuLdGYT8A15hgtnLo9/7PiB0rJiDGB5D+537x4tEor3HUiTyWpwHD98ZO/vZ09Nm/pWv7iBYpFYKq0+cnQfAFhdneyHH74pKS1avmyNn18Ay8XlyNF9+QW5QUEhnh5eM2am1tXJ5PK6I0f3GQ2Gt9+ax2J19Mqh8I46JIovbOVrw0KjwuVSE8/NznckGIa5u/nduH08L/8SAYiyx/ePnPjabDYGd4kFAJy7tDPQP7J7+JNlzbJuHuVyBX16vuLtGXov9+ztOyd1eo1GW3/t5pGikluB/lHRkQn2jQcA0Ku0odFciY+NC3q7KSgWib28fC5cOHPt2qWGBvXo0ePCwyPc3SXnzmeeOn1cWa+YOnXW9GlvWR9M9YsbpNU8IWSvAAADj0lEQVRqTp0+du5choAvWLZ0db9+gwAAIqHIz9f/zzs3GRgjKjq2oqL88pXzly6f8/Dw+mDF2oCAwI7noaeCfDHrxm91HsH2v/zy8QoJDIguLs2+nX2yvCLXzy+8b+8x1n7B1hRkMBhREQmyurJ7uWeLS7N9vcMU9VU+XqFkKFhyu2bUNB8Gw8ZjSdsra93IUBj1oNdwOi5N3EFObqsYlurpS7/FjX794rFbkAff1YkekDTUNZrUDSnzbQ+OpFcj4QxEDxQ+ytW1oeDDRzd27lv59Os8rqi1ruNxoxcOjEu2V8IHBVd2H1zz9OsEQQBA2Oy4mTPrv4H+ka0VaNAYevQXtPYuUpBqeg91v3aiyD1QzGTZvhcMCeq5ZN6up18nCNDa8Bo+z55n9q6hfW0GsFgsBEHY3EdcLPJqrTSjDldLNVH9Wl1ODikIgfjxHnm3Fb7dbXTaAQDYbK6EDXNAv30D1BXXD0n2aOMANGQVAj2HuPG4ZoOunU6TToC+weDmgbU9uR0pCIcxs3yLsyphpyAXi4UovlGVNMu37cOQgnBgcxjJc/1LbnRmC4uzKqasCGr3MKQgNPxCeakLfEtuVMAOYn/MJkvhlfKp7we6e7c/uAQpCBNXD/b42b45mSU6dedZGVtbry+8XD55SSBf2KGbXaQgZDwDOPM3drVo1JU5NQYtFSMGyEOnNjy+W+1i0cz5vKu4w6vko04Z+GAYNvZtv5Ic7R9HavluXBafI/biMx1nlrHJYFbLtGaDEdcahqd6dol4thUvkYJ0ITRGEBojKLqvKbyjfXRFIQnk4wYLk81icVg0XLGYIAizwWTGTS5sRr1UFxoj6BYvDIl+nmURkYL0omussGusEABQXaLTqsxalclosOjtsdCvfeHwGVw+my/mi9yZPkHtdLu0DVKQpviFkjLFhIbYVpDNxSz0a/yfCVcvF9ImQiDsie3fksjdRVbm2OsilNzTePh1hhlPnR7bCnp34dByzZOOopQZQ3rwWS6oGXQAWm0FA8K5fxySUp7HPpzdXTUwqa3RGQj60NZ+xLnXVIXZml7DPNx92K0NbqMVOo1JVYf/cVD66sIAtw48GkLQgXa2xC7J1WZfVEpL9EwW3U/MEj+OSmYMi+H3H+MhEKM7fYehHQWbMOjoviUdQQAu3wGaakQLOqogAkESqNlAQAYpiIAMUhABGaQgAjJIQQRkkIIIyPx/ohlWIXXfCHUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
        "messages = react_graph.invoke({\"messages\": messages})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDzcubxICeRI",
        "outputId": "802ae4f6-fd10-4964-80c0-affff1c4b95c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Add 3 and 4.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  add (6298c680-9aed-4f68-b410-4391149f57ea)\n",
            " Call ID: 6298c680-9aed-4f68-b410-4391149f57ea\n",
            "  Args:\n",
            "    a: 3.0\n",
            "    b: 4.0\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: add\n",
            "\n",
            "7\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The sum of 3 and 4 is 7.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [HumanMessage(content=\"Multiply that by 2.\")]\n",
        "messages = react_graph.invoke({\"messages\": messages})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gId8rtXSIW3T",
        "outputId": "0d4e92a7-58f9-4dbe-942c-680c3a4ac275"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiply that by 2.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Multiply what by 2?  I need more information to perform the calculation. Please provide the number you want me to multiply.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "memory: MemorySaver = MemorySaver()\n",
        "react_graph_memory: CompiledStateGraph = builder.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "pBYNEN4DIceE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify a thread\n",
        "config1 = {\"configurable\": {\"thread_id\": \"1\"}}\n"
      ],
      "metadata": {
        "id": "XgqnnKh9Ietc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Specify an input\n",
        "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
        "\n",
        "# Run\n",
        "messages = react_graph_memory.invoke({\"messages\": messages},config1)\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KSDFLmTIjEI",
        "outputId": "f247b648-e901-4141-d725-8e3caaa7b586"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Add 3 and 4.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  add (538b6249-cd88-4237-8de3-6d9b0461d29c)\n",
            " Call ID: 538b6249-cd88-4237-8de3-6d9b0461d29c\n",
            "  Args:\n",
            "    a: 3.0\n",
            "    b: 4.0\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: add\n",
            "\n",
            "7\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The sum of 3 and 4 is 7.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [HumanMessage(content=\"Multiply that by 2.\")]\n",
        "messages = react_graph_memory.invoke({\"messages\": messages}, config1)\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "id": "KHYSAqjoIlot",
        "outputId": "41c30fa0-986b-456e-8f67-3dfa27c20f3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Add 3 and 4.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  add (538b6249-cd88-4237-8de3-6d9b0461d29c)\n",
            " Call ID: 538b6249-cd88-4237-8de3-6d9b0461d29c\n",
            "  Args:\n",
            "    a: 3.0\n",
            "    b: 4.0\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: add\n",
            "\n",
            "7\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The sum of 3 and 4 is 7.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiply that by 2.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  multiply (f3257e3d-607a-4495-af0a-b8ef764391d4)\n",
            " Call ID: f3257e3d-607a-4495-af0a-b8ef764391d4\n",
            "  Args:\n",
            "    a: 7.0\n",
            "    b: 2.0\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: multiply\n",
            "\n",
            "14\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The result of multiplying 7 by 2 is 14.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiply that by 2.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  multiply (e6f11c1d-db09-4b69-ba14-f019e9c9d5fa)\n",
            " Call ID: e6f11c1d-db09-4b69-ba14-f019e9c9d5fa\n",
            "  Args:\n",
            "    a: 14.0\n",
            "    b: 2.0\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: multiply\n",
            "\n",
            "28\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "14 multiplied by 2 is 28.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import START, StateGraph\n",
        "from langgraph.prebuilt import tools_condition, ToolNode\n",
        "from IPython.display import Image, display\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "\n",
        "# Initialize the graph builder\n",
        "builder: StateGraph = StateGraph(MessagesState)\n",
        "\n",
        "# Define nodes and edges\n",
        "builder.add_node(\"assistant\", assistant)\n",
        "builder.add_node(\"tools\", ToolNode(tools))\n",
        "builder.add_edge(START, \"assistant\")\n",
        "builder.add_conditional_edges(\n",
        "    \"assistant\",\n",
        "    tools_condition,\n",
        ")\n",
        "builder.add_edge(\"tools\", \"assistant\")\n",
        "\n",
        "# Compile the graph\n",
        "react_graph: CompiledStateGraph = builder.compile()\n",
        "\n",
        "# Display the graph structure (optional)\n",
        "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))\n",
        "\n",
        "# Define a function to handle user input and respond\n",
        "def start_chat():\n",
        "    print(\"Welcome to the chat! Type something to start talking (type 'quit' to stop).\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == 'quit':\n",
        "            break\n",
        "        # Create a message and invoke the graph\n",
        "        messages = [HumanMessage(content=user_input)]\n",
        "        responses = react_graph.invoke({\"messages\": messages})\n",
        "        # Print responses\n",
        "        for response in responses['messages']:\n",
        "            print(\"Bot:\", response.content)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_chat()\n"
      ],
      "metadata": {
        "id": "jtSx-SYEIpYB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f1ede1a6-f299-4e36-aef5-5e035955dee8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the chat! Type something to start talking (type 'quit' to stop).\n",
            "You: hi\n",
            "Bot: hi\n",
            "Bot: Hello! How can I help you today?\n",
            "\n",
            "You: can you please tell me cousrez outline\n",
            "Bot: can you please tell me cousrez outline\n",
            "Bot: The Panaversity AI program is structured into three levels: Core, Professional, and Vertical Specialization.  The Core level consists of five courses:\n",
            "\n",
            "**I. Autonomous Agentic and Robotic AI Core Level**\n",
            "\n",
            "*   **AI-101: Modern AI Python Programming:**  Focuses on mastering fundamental modern Python, including typing, and using AI to write Python programs.  It also introduces GenAI, prompt engineering, and basic Linux, Docker, VSCode, Devcontainer, and GitHub.  Certification: Certified Professional Python Programmer (CPPP1).\n",
            "\n",
            "*   **AI-201: Fundamentals of Agentic AI:** Covers Generative AI, Agentic AI, custom GPT development, prompt engineering using CrewAI, Knowledge Graph construction with Graph Databases and GQL, and Agentic Payments.  Prerequisite: AI-101.\n",
            "\n",
            "*   **AI-202: Advanced Agentic AI Engineering:** Builds on AI-201, focusing on LangGraph and LangChain for complex AI agent development.  It also introduces frontend development with Next.js and TypeScript. Prerequisites: AI-101, AI-201.\n",
            "\n",
            "*   **AI-301: Cloud Native AI Microservices:** Teaches building scalable AI-powered APIs using FastAPI, GQL, Neo4j, Kafka, Kong, GenAI APIs (OpenAI), LangChain, open-source LLMs, containers, DevContainers, and deployment using Docker Compose and Kubernetes.  It also integrates design thinking and BDD. Certifications: Neo4j Certified Professional, Confluent Certified Developer for Apache Kafka (CCDAK), Design Thinking Professional Certificate (DTPC), Test and Behavior Driven Development (TDD/BDD). Prerequisite: AI-101.\n",
            "\n",
            "*   **AI-451: Physical and Humanoid Robotics AI:**  Explores humanoid robotics using ROS 2, Gazebo Robot Simulator, and NVIDIA Isaac.  It covers ROS 2 for robotic control, simulations with Gazebo and Unity, and using OpenAI’s GPT models for conversational AI. Prerequisite: AI-101.\n",
            "\n",
            "*   **AI-461: Distributed AI Computing:**  Covers distributed computing using Ray, focusing on its application in machine learning, data processing, and reinforcement learning. Prerequisite: AI-101.\n",
            "\n",
            "\n",
            "**II. Professional Level**\n",
            "\n",
            "*   **AI-500: AI Ethics and Governance: Principles and Practices**\n",
            "\n",
            "*   **AI-501: Distributed Machine Learning:**  Focuses on building and training AI models using Python, PyTorch, and Ray.  Covers GANs, Transformers, LLMs, variational autoencoders, and diffusion models. Prerequisites: AI-101, AI-461.\n",
            "\n",
            "*   **AI-502: Customising Open Source LLMs:** Covers fine-tuning open-source LLMs like Meta LLaMA 3 using PyTorch, focusing on cloud-native training and deployment with Nvidia NIM, Docker, and Kubernetes. Prerequisites: AI-101, AI-461, AI-501.\n",
            "\n",
            "*   **AI-651: Advanced Cloud Native and Distributed AI Computing:** Masters Kubernetes, Ray, Terraform, and GitHub Actions for deploying AI pipelines, APIs, microservices, and open-source models.  Covers distributed system design. Certifications: Certified Kubernetes Application Developer (CKAD), HashiCorp Certified: Terraform Associate. Prerequisites: AI-101, AI-301, AI-461.\n",
            "\n",
            "\n",
            "**III. Vertical Specialization Level (Optional)**\n",
            "\n",
            "Eight specializations are offered at the end of the program, focusing on applying AI to specific industries or domains.  Details of each specialization are available in the FAQs.\n",
            "\n",
            "\n",
            "This is a comprehensive overview.  Specific details for each course, including learning repositories and chapter breakdowns, are available in the provided knowledge base.\n",
            "\n",
            "You: tell me in two lines\n",
            "Bot: tell me in two lines\n",
            "Bot: Panaversity offers a comprehensive 18-month Certified Agentic and Robotic AI Engineer program,  covering Agentic AI, Physical AI, and Cloud Native technologies.  Students can accelerate their learning by taking multiple courses concurrently.\n",
            "\n",
            "You: quiet\n",
            "Bot: quiet\n",
            "Bot: OK.  I will be quiet and wait for your next question.\n",
            "\n",
            "You: exit\n",
            "Bot: exit\n",
            "Bot: OK. Goodbye!\n",
            "\n",
            "You: quit\n"
          ]
        }
      ]
    }
  ]
}